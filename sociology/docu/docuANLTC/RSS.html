<!doctype html>
<html>
<head>
<meta charset="utf-8">
<title>CIS 2760 Analytics</title>
<link href="css/2760css.css" rel="stylesheet" type="text/css">
</head>

<body>

<!-- Google Tag Manager -->
<noscript><iframe src="//www.googletagmanager.com/ns.html?id=GTM-PWZWZR"
height="0" width="0" style="display:none;visibility:hidden"></iframe></noscript>
<script>(function(w,d,s,l,i){w[l]=w[l]||[];w[l].push({'gtm.start':
new Date().getTime(),event:'gtm.js'});var f=d.getElementsByTagName(s)[0],
j=d.createElement(s),dl=l!='dataLayer'?'&l='+l:'';j.async=true;j.src=
'//www.googletagmanager.com/gtm.js?id='+i+dl;f.parentNode.insertBefore(j,f);
})(window,document,'script','dataLayer','GTM-PWZWZR');</script>
<!-- End Google Tag Manager -->
<header id="head">
  <p><strong> CIS 2760 Web Analytics</strong></p>

</header>
<div class="links">
<ul>
	<li><a href="http://www.w3schools.com" target="_blank">W3 schools</a></li> 
	<li><a href="http://validator.w3.org" target="_blank">XHTML validator</a></li>
	<li><a href="http://jigsaw.w3.org/css-validator/" target="_blank">CSS validator</a></li>
	<li><a href="http://jqueryui.com" target="_blank">jQueryUI</a></li>
	<li><a href="http://www.dynamicdrive.com/" target="_blank">Dynamic Drive</a></li>
	<li><a href="http://php.net" target="_blank">PHP.net</a></li>
	<li><a href="http://support.google.com/webmasters/?hl=en" target="_blank">Google Practices</a></li>
	<li><a href="http://developer.yahoo.com/" target="_blank">Yahoo! Developer</a></li>
	<li><a href="http://www.useit.com/" target="_blank">Jakob Nielson</a></li>
	<li><a href="http://www.alistapart.com/" target="_blank">A List Apart</a></li>
	<li><a href="http://webreference.com/" target="_blank">Web Reference</a></li>
	<li><a href="pdf/Chapter_B.pdf" target="_blank">Frames Tutorial</a></li>
	<li><a href="http://layersmagazine.com/category/dreamweaver" target="_blank">Layers Magazine</a></li>
	<li><a href="https://www.youtube.com/watch?v=WmUApIVc2W0&amp;list=PL58D802E3FC11F80F" target="_blank">
			DW Tutorial1</a></li>
	<li><a href="http://www.dreamweavertutorial.co.uk" target="_blank">DW Tutorial2</a></li>
</ul></div>
<div class="container">
<div id="rightmenu">
  <nav class="menu2" id="first2"></nav>
  <nav class="menu2"></nav>
  <nav class="menu2"> </nav>
<nav class="menu2"> </nav>
<nav class="menu2"> </nav>
<nav class="menu2"> </nav>
<nav class="menu2"></nav>
<nav class="menu2"></nav>
<nav class="menu2"></nav>
<nav class="menu2"></nav>
<nav class="menu2" id="last2"></nav>
<div>
This XML file does not appear to have any style information associated with it. The document tree is shown below.
<feed xmlns="http://www.w3.org/2005/Atom" xmlns:geo="http://www.w3.org/2003/01/geo/wgs84_pos#" xmlns:feedburner="http://rssnamespace.org/feedburner/ext/1.0" xml:lang="en">
<title type="text">igvita.com</title>
<link rel="alternate" type="text/html" href="http://www.igvita.com"/>
<atom10:link xmlns:atom10="http://www.w3.org/2005/Atom" rel="self" type="application/atom+xml" href="http://feeds.igvita.com/igvita"/>
<subtitle type="text">a goal is a dream with a deadline</subtitle>
<author>
<name>Ilya Grigorik</name>
<email>ilya@igvita.com</email>
<uri>http://www.igvita.com/</uri>
</author>
<updated>2014-05-08T00:56:18+00:00</updated>
<feedburner:info uri="igvita"/>
<atom10:link xmlns:atom10="http://www.w3.org/2005/Atom" rel="hub" href="http://pubsubhubbub.appspot.com/"/>
<id>http://www.igvita.com/</id>
<geo:lat>37.40679</geo:lat>
<geo:long>-122.074613</geo:long>
<entry>
<title type="html">Minimum Viable Block Chain</title>
<link rel="alternate" type="text/html" href="http://feeds.igvita.com/~r/igvita/~3/d0OcUg0P9pw/"/>
<updated>2014-05-05T00:00:00-07:00</updated>
<id>
http://www.igvita.com/2014/05/05/minimum-viable-block-chain/
</id>
<content type="html">
<p><img src='http://www.igvita.com/posts/14/blockchain.png' class='left' style='max-width:300px;width:100%' /> Cryptocurrencies, and Bitcoin in particular, have been getting a lot of attention from just about every angle: regulation, governance, taxation, technology, product innovation, and the list goes on. The very concept of a "peer-to-peer (decentralized) electronic cash system" turns many of our previously held assumptions about money and finance on their head.</p> <p>That said, putting the digital currency aspects aside, an arguably even more interesting and far-reaching innovation is the underlying block chain technology. Regardless of what you think of Bitcoin, or its <a href="https://en.bitcoin.it/wiki/List_of_alternative_cryptocurrencies">altcoin derivatives</a>, as a currency and a store of value, behind the scenes they are all operating on the same basic block chain principles <a href="https://bitcoin.org/bitcoin.pdf">outlined by Satoshi Nakamoto</a>:</p> <blockquote> We propose a solution to the double-spending problem using a peer-to-peer network. The network timestamps transactions by hashing them into an ongoing chain of hash-based proof-of-work, forming a record that cannot be changed without redoing the proof-of-work. The longest chain not only serves as proof of the sequence of events witnessed, but proof that it came from the largest pool of CPU power... The network itself requires minimal structure. </blockquote> <p><strong>The block chain is agnostic to any "currency". In fact, it can (and will) be adapted to power many other use cases.</strong> As a result, it pays to understand the how and the why behind the "minimum viable block chain":</p> <ul> <li><strong>What follows is not</strong> an analysis of the Bitcoin block chain. In fact, I intentionally omit mentioning both the currency aspects, and the many additional features that the Bitcoin block chain is using in production today.</li> <li><strong>What follows is</strong> an attempt to explain, from the ground up, why the particular pieces (digital signatures, proof-of-work, transaction blocks) are needed, and how they all come together to form the "minimum viable block chain" with all of its remarkable properties.</li> </ul> <div class="callout"> I have learned long ago that writing helps me refine my own sloppy thinking, hence this document. Primarily written for my own benefit, but hopefully helpful to someone else as well. Feedback is always welcome, leave a comment below! </div> <hr/> <ul class="post-toc" id="toc"> <li><a href="#triple-entry">Securing transactions with triple-entry bookkeeping</a> <li><a href="#pki">Securing transactions with PKI</a> <li><a href="#balance">Balance = Î£(receipts)</a> <li><a href="#multiparty">Multi-party transfers & verification</a> <li><a href="#doublespend">Double-spending and distributed consensus </a> <ul style="margin:0em"> <li><a href="#requirements">Requirements for a distributed consensus network</a> <li><a href="#sybil">Protecting the network from Sybil attacks </a> <li><a href="#pow">Proof-of-work as a participation requirement</a> </ul> <li><a href="#mvb">Building the minimum viable block chain</a> <ul style="margin:0em"> <li><a href="#blocks">Adding "blocks" & transaction fee incentives</a> <li><a href="#incentives">Racing to claim the transaction fees</a> <li><a href="#conflicts">Resolving chain conflicts</a> <li><a href="#guarantees">Blocks are never final</a> </ul> <li><a href="#properties">Properties of the (minimum viable) block chain</a> </ul> <hr /> <h2 id="triple-entry" style="margin-top:1em;">Securing transactions with triple-entry bookkeeping <a href="#triple-entry">#</a> <a class="toc-arrow" href="#toc">&uarr;</a></h2> <p>Alice and Bob are stamp collectors. It's nothing serious, and they're mostly in it for the social aspects of meeting others, exchanging stories, and doing an occasional trade. If both parties see something they like, they negotiate right there and then and complete the swap. In other words, it's a simple <a href="https://en.wikipedia.org/wiki/Barter">barter system</a>.</p> <p>Then, one day Bob shows up with a stamp that Alice feels she absolutely must have in her collection. Except there is a problem, because Bob is not particularly interested in anything that Alice has to offer. Distraught, Alice continues negotiating with Bob and they arrive at a solution: they'll do a one-sided transaction where Bob will give Alice his stamp and Alice will promise to repay him in the future.</p> <p>Both Bob and Alice have known each other for a while, but to ensure that both live up to their promise (well, mostly Alice), they agree to get their transaction "notarized" by their friend Chuck.</p> <p><img src='http://www.igvita.com/posts/14/transaction-signatures.png' class='center' style='max-width:443px;width:100%' /></p> <p>They make three copies (one for each party) of the above transaction receipt indicating that Bob gave Alice a "Red stamp". Both Bob and Alice can use their receipts to keep account of their trade(s), and Chuck stores his copy as evidence of the transaction. Simple setup but also one with a number of great properties:</p> <ol> <li><strong>Chuck can authenticate both Alice and Bob</strong> to ensure that a malicious party is not attempting to fake a transaction without their knowledge.</li> <li><strong>The presence of the receipt in Chuck's books is proof of the transaction.</strong> If Alice claims the transaction never took place then Bob can go to Chuck and ask for his receipt to disprove Alice's claim.</li> <li><strong>The absence of the receipt in Chuck's books is proof that the transaction never took place.</strong> Neither Alice nor Bob can fake a transaction. They may be able to fake their copy of the receipt and claim that the other party is lying, but once again, they can go to Chuck and check his books.</li> <li><strong>Neither Alice nor Bob can tamper with an existing transaction.</strong> If either of them does, they can go to Chuck and verify their copies against the one stored in his books.</li> </ol> <p>What we have above is an implementation of "triple-entry bookkeeping", which is simple to implement and offers good protection for both participants. Except, of course you've already spotted the weakness, right? We've placed a lot of trust in an intermediary. If Chuck decides to collude with either party, then the entire system falls apart.</p> <p>Moral of the story? <strong>Be (very) careful about your choice of the intermediary!</strong></p> <h2 id="pki">Securing transactions with PKI <a href="#pki">#</a> <a class="toc-arrow" href="#toc">&uarr;</a></h2> <p>Dissatisfied with the dangers of using a "reliably intermediary", Bob decides to do some research and discovers that public key cryptography can eliminate the need for an intermediary! This warrants some explanationâ¦</p> <blockquote cite="https://en.wikipedia.org/wiki/Public-key_cryptography">Public-key cryptography, also known as asymmetric cryptography, refers to a cryptographic algorithm which requires two separate keys, one of which is secret (or private) and one of which is public. Although different, the two parts of this key pair are mathematically linked. The public key is used to encrypt plaintext or to verify a digital signature; whereas the private key is used to decrypt ciphertext or to create a digital signature. </blockquote> <p>The original intent behind using a third party (Chuck) was to ensure three properties:</p> <ul> <li><strong>Authentication:</strong> a malicious party can't masquerade as someone else.</li> <li><strong>Non-repudiation:</strong> participants can't claim that the transaction did not happen after the fact.</li> <li><strong>Integrity:</strong> the transaction receipt can't be modified after the fact.</li> </ul> <p>Turns out, public key cryptography can satisfy all of the above requirements. Briefly, the workflow is as follows:</p> <p><img src='http://www.igvita.com/posts/14/transaction-pki.png' class='center' style='max-width:464px;width:100%' /></p> <ol> <li>Both Alice and Bob generate a set public-private keypairs.</li> <li>Both Alice and Bob publish their public keys to the world.</li> <li>Alice writes a transaction receipt in plaintext.</li> <li>Alice encrypts the plaintext of the transaction using her private key.</li> <li>Alice prepends a plaintext "signed by" note to the ciphertext.</li> <li>Both Alice and Bob store the resulting output.</li> </ol> <div class="callout"> Note that step #5 is only required when many parties are involved: if you don't know who signed the message then you don't know whose public key you should be using to decrypt it. This will become relevant very soon... </div> <p>This may seem like a lot of work for no particular reason, but let's examine the properties of our new receipt:</p> <ol> <li>Bob doesn't know Alice's private key, but that doesn't matter because he can look up her public key (which is shared with the world) and use it to decrypt the ciphertext of the transaction.</li> <li>Alice is not really "encrypting" the contents of the transaction. Instead, by using her private key to encode the transaction she is "signing it": anyone can decrypt the ciphertext by using her public key, and because she is the only one in possession of the private key this mechanism guarantees that only she could have generated the ciphertext of the transaction.</li> </ol> <div class="callout"> How does Bob, or anyone else for that matter, get Alice's public key? There are many ways to handle distribution of public keys - e.g. Alice publishes it it on her website. We'll assume that some such mechanism is in place. </div> <p>As a result, the use of public key infrastructure (PKI) fulfills all of our earlier requirements:</p> <ol> <li>Bob can use Alice's public key to authenticate signed transaction by decrypting the ciphertext.</li> <li>Only Alice knows her private key, hence Alice can't deny that the transaction took place - she signed it.</li> <li>Neither Bob nor anyone else can fake or modify a transaction without access to Alice's private key.</li> </ol> <div class="callout"> Note that for #2, Alice can deny that she is the true owner of the public-private keypair in question - i.e. someone is faking her identity in #1. The keypair to identity association is something your key distribution mechanisms needs to account for. </div> <p><strong>Both Alice and Bob simply store a copy of the signed transaction and the need for an intermediary is eliminated.</strong> The "magic" of public key cryptography is a perfect match for their two-party barter system.</p> <h2 id="balance">Balance = Î£(receipts) <a href="#balance">#</a> <a class="toc-arrow" href="#toc">&uarr;</a></h2> <p>With the PKI infrastructure in place, Bob and Alice complete a few additional trades: Alice acquires another stamp from Bob and Bob picks up a stamp from Alice. They each follow the same steps as before to generate signed transactions and append them to their respective ledgers.</p> <p><img src='http://www.igvita.com/posts/14/signed-ledger.png' class='center' style='max-width:618px;width:100%' /></p> <p>The records are secure, but there is a small problem: it's not clear if either party has an outstanding balance. Previously, with just one transaction, it was clear who owed whom (Alice owed Bob) and how much (one red stamp), but with multiple transactions the picture gets really murky. Are all stamps of equal value? If so, then Alice has a negative balance. If not, then it's anyone's guess! To resolve this, Alice and Bob agree on the following:</p> <ul> <li>Yellow stamp is worth twice the value of a red stamp.</li> <li>Blue stamp is equal in value to a red stamp.</li> </ul> <p>Finally, to ensure that their new agreement is secure they regenerate their ledgers by updating each transaction with its relative value. Their new ledgers now look as follows:</p> <p><img src='http://www.igvita.com/posts/14/signed-ledger-value.png' class='center' style='max-width:687px;width:100%' /></p> <p>With that, computing the final balance is now a simple matter of iterating through all of the transactions and applying the appropriate debits and credits to each side. The net result is that Alice owes Bob 2... <em>units of value</em>. What's a "unit of value"? It's an arbitrary <a href="https://en.wikipedia.org/wiki/Medium_of_exchange">medium of exchange</a> that Alice and Bob have agreed on. Further, since "unit of value" doesn't roll off the tongue, Alice and Bob agree to call 1 unit of value as 1 <em>chroma</em> (plural: <em>chroms</em>).</p> <p>All of the above seems trivial, but <strong>the fact that the balance of each party is a function of all of the receipts in the ledger has an important consequence: anyone can compute everyone's balance.</strong> There is no need for any trusted intermediaries and the system is trivial to audit. Anyone can traverse the full ledger, verify the trades, and figure out the outstanding balances of each party.</p> <h2 id="multiparty">Multi-party transfers & verification <a href="#multiparty">#</a> <a class="toc-arrow" href="#toc">&uarr;</a></h2> <p>Next, Bob stumbles across a stamp owned by John that he really likes. He tells John about the secure ledger he is using with Alice and asks him if he would be willing to do a trade where Bob transfers his balance with Alice as a method of payment - i.e. Bob gets the stamp from John, and Alice would owe John the amount she previously owed Bob. John agrees, but now they have dilemma. How exactly does Bob "transfer" his balance to John in a secure and verifiable manner? After some deliberation, they arrive at an ingenious plan:</p> <p><img src='http://www.igvita.com/posts/14/signed-ledger-transfer.png' class='center' style='max-width:695px;width:100%' /></p> <p>Bob creates a new transaction by following the same procedure as previously, except that he first computes the SHA-256 checksum (a unique fingerprint) of the encrypted transaction he wants to transfer and then inserts the checksum in the "What" field of the new receipt. In effect, he is linking the new transfer receipt to his previous transaction with Alice, and by doing so, transfers its value to John.</p> <div class="callout"> To keep things simple, we'll assume that all transfers "spend" full value of the transaction being transfered. It's not too hard to extend this system to allow fractional transfers, but that's unnecessary complexity at this point. </div> <p>With the new transaction in place, John makes a copy of the encrypted ledger for his safekeeping (now there are three copies) and runs some checks to verify its integrity:</p> <ol> <li>John fetches Alice's and Bob's public keys and verifies the first three transactions.</li> <li>John verifies that Bob is transferring a "valid" transaction: <ul> <li>The transaction that is being transferred is addressed to Bob.</li> <li>Bob has not previously transfered the same transaction to anyone else.</li> </ul> </li> </ol> <p>If all the checks pass, they complete the exchange and we can compute the new balances by traversing the ledger: Bob has a net zero balance, Alice has a debit of 2 chroms, and John has a credit of 2 chroms (courtesy of Alice). Further, John can now take his new ledger to Alice and ask her for payment, and even though Alice wasn't present for their transaction, that's not a problem:</p> <ul> <li>Alice can verify the signature of the new transfer transaction using Bob's public key.</li> <li>Alice can verify that the transfer transaction is referencing one of her own valid transactions with Bob.</li> </ul> <div class="callout"> The above transfer and verification process is a pretty remarkable property of the system! Note that to make it all work, we need two enabling technologies: (a) use of PKI, which enables digital signature verification, and (b) the receipt ledger, which enables us to look at the full transaction history to verify balances and to link previous transactions to enable the transfer. </div> <p>Satisfied with their ingenuity John and Bob part ways: Bob goes home with a new stamp and John with a new ledger. On the surface, everything looks great, but <strong>they've just exposed themselves to a challenging security problem... Can you spot it?</strong></p> <h2 id="doublespend">Double-spending and distributed consensus <a href="#doublespend">#</a> <a class="toc-arrow" href="#toc">&uarr;</a></h2> <p>Shortly after completing the transaction with John, Bob realizes that they have just introduced a critical flaw into their system and one that he could exploit to his advantage if he acts quickly: both Bob and John have updated their ledgers to include the new transaction, but neither Alice nor anyone else is aware that it has taken place. As a result, <strong>there is nothing stopping Bob from approaching other individuals in his network and presenting them with an old copy of the ledger that omits his transaction with John!</strong> If he convinces them to do a transaction, just as he did with John, then he can "<a href="https://en.wikipedia.org/wiki/Double-spending">double-spend</a>" the same transaction as many times as he wants!</p> <p><img src='http://www.igvita.com/posts/14/doublespend.png' class='center' style='max-width:690px;width:100%' /></p> <p>Of course, once multiple people show up with their new ledgers and ask Alice for payment, the fraud <em>will</em> be detected, but that is of little consolation - Bob has already run away with his loot!</p> <p>The double-spend attack was not possible when we only had two participants since in order to complete the transaction you'd verify and update both sets of books simultaneously. As a result, all ledgers were always in sync. However, the moment we added an extra participant we introduced the possibility of incomplete and inconsistent ledgers between all the participants, which is why the double-spend is now possible.</p> <div class="callout"> In CS speak, a two-party ledger provides "strong consistency", and growing the ledger beyond two parties requires some form of <a href="https://en.wikipedia.org/wiki/Consensus_(computer_science)">distributed consensus</a> to mitigate double-spend. </div> <p><strong>The simplest possible solution to this problem is to require that all parties listed in the ledger must be present at the time when each new transaction is made, such that everyone can update their books simultaneously.</strong> An effective strategy for a small-sized group, but also not a scalable one for a large number of participants.</p> <h3 id="requirements">Requirements for a distributed consensus network <a href="#requirements">#</a> <a class="toc-arrow" href="#toc">&uarr;</a></h3> <p>Let's imagine that we want to scale our ledger to all stamp collectors around the globe such that anyone can trade their favorite stamps in a secure manner. Obviously, requiring that every participant must be present to register each transaction would never work due to geography, timezones, and other limitations. Can we build a system where we don't need everyone's presence and approval?</p> <ol> <li>Geography is not really an issue: we can move communication online.</li> <li>Timezone problems can be solved with software: we don't need each individual to manually update their ledgers. Instead, we can build software that can run on each participant's computer and automatically receive, approve, and add transactions to the ledger on their behalf.</li> </ol> <p>In effect, we could build a peer-to-peer (P2P) network that would be responsible for distributing new transactions and getting everyone's approval! Except, unfortunately that's easier said than done in practice. For example, <strong>while a P2P network can resolve our geography and timezone problems, what happens when even just one of the participants goes offline?</strong> Do we block all transactions until they're back online?</p> <div class="callout"> Note that the "how" of building a P2P network is a massive subject in its own right: protocols and signaling, traversing firewalls and NATs, bootstrapping, optimizing how updates are propagated, security, and so on. That said, the low-level mechanics of building such a network are out of scope of our discussion... we'll leave that as an exercise for the reader. </div> <p><img src='http://www.igvita.com/posts/14/p2p-overlay.png' class='left' />Turns out, <a href="https://en.wikipedia.org/wiki/Consensus_(computer_science)">distributed consensus</a> is a well studied problem in computer science, and one that offers some promising solutions. For example, <a href="https://en.wikipedia.org/wiki/Two-phase_commit_protocol">two-phase commit</a> (2PC) and <a href="https://en.wikipedia.org/wiki/Paxos_(computer_science)">Paxos</a> both enable a mechanism where we only need the majority quorum (50%+) of participants to be present to safely commit a new transaction: as long as the majority has accepted the transaction the remainder of the group is guaranteed to eventually converge on the same transaction history.</p> <p><strong>That said, neither 2PC nor Paxos are sufficient on their own.</strong> For example, how would either 2PC or Paxos know the total number of participants in our P2P stamp-collector network when new participants are joining on a daily basis and others are disappearing without notice? If one of the prior participants is offline, are they offline temporarily, or have they permanently left the network? Similarly, there is another and an even more challenging "<a href="https://en.wikipedia.org/wiki/Sybil_attack">Sybil attack</a>" that we must account for: there is nothing stopping a malicious participant from creating many profiles to gain an unfair share of voting power within our P2P network.</p> <p>If the number of participants in the system was fixed and their identities were authenticated and verified (i.e. a trusted network), then both 2PC and Paxos would work really well. Alas, that is simply not the case in our ever changing stamp collector P2P network. Have we arrived at a dead end? Well, not quiteâ¦</p> <p><strong>One obvious solution to solve our dilemma is to eliminate the "distributed" part from the problem statement.</strong> Instead of building a P2P distributed system we could, instead, build a global registry of all stamp collectors that would record their account information, authenticate them and (try to) ensure that nobody is cheating by creating multiple identities, and most importantly, keep one shared copy of the ledger! Concretely, <strong>we could build a website where these trades can take place, and the website would then take care of ensuring the integrity and correct ordering of all transactions by recording them in its centralized database.</strong></p> <p>The above is a practical solution but, let's admit it, an unsatisfying one since it forces us to forfeit the peer-to-peer nature of our ledger system. It places all of the trust in a single centralized system and opens up an entirely new set of questions: what is the uptime, security and redundancy of the system; who maintains the system and what are their incentives; who has administrative access, and so on. <strong>Centralization brings its own set challenges.</strong></p> <p>Let's rewind and revisit some of the problems we've encountered with our P2P design:</p> <ul> <li>Ensuring that every participant is always up to date (strongly consistent system) imposes high coordination costs and affects availability: if a single peer is unreachable the system cannot commit new transactions.</li> <li>In practice we don't know the global status of the P2P network: number of participants, whether individuals are temporarily offline or decided to leave the network, etc.</li> <li>Assuming we can resolve the above constraints, the system is still open to a Sybil attack where a malicious user can create many fake identities and exercise unfair voting power.</li> </ul> <p><strong>Unfortunately, resolving all of the above constraints is impossible unless we relax some of the requirements:</strong> the <a href="https://en.wikipedia.org/wiki/CAP_theorem">CAP theorem</a> tells us that our distributed system can't have strong consistency, availability, and partition tolerance. As a result, in practice our P2P system <a href="http://www.igvita.com/2010/06/24/weak-consistency-and-cap-implications/">must operate under the assumption of weak(er) consistency</a> and deal with its implications:</p> <ul> <li>We must accept that some ledgers will be out of sync (at least temporarily).</li> <li>The system must eventually converge on a global ordering (linearizability) of all transactions.</li> <li>The system must resolve ledger conflicts in a predictable manner.</li> <li>The system must enforce global invariants - e.g. no double-spends.</li> <li>The system should be secure against Sybil and similar attacks.</li> </ul> <h3 id="sybil">Protecting the network from Sybil attacks <a href="#sybil">#</a> <a class="toc-arrow" href="#toc">&uarr;</a></h3> <p>Achieving consensus in a distributed system, say by counting votes of each participant, opens up many questions about the "voting power" of each peer: who is allowed to participate, do certain peers have more voting power, is everyone equal, and how do we enforce these rules?</p> <p>To keep things simple, let's say everyone's vote is equal. As a first step, we could require that each participant sign their vote with their private key, just as they would a transaction receipt, and circulate it to their peers - signing a vote ensures that someone else can't submit a vote on their behalf. Further, we could make a rule that only one vote is allowed to be submitted. If multiple votes are signed by the same key then all of them are discarded - make up your mind already! So far so good, and now the hard partâ¦</p> <p><a href="http://www.identitywoman.net/the-identity-spectrum"><img src="/posts/14/identity-spectrum.png" class="center" style="max-width:559px;width:100%"></a></p> <p>How do we know if any particular peer is allowed to participate in the first place? If all that's needed is just a unique private key to sign a vote, then a malicious user could simply generate an unlimited number of new keys and flood the network. The root problem is that <strong>when forged identities are cheap to generate and use, any voting system is easily subverted.</strong></p> <p><strong>To solve this problem we need to make the process of submitting the vote "expensive".</strong> Either the cost of generating a new identity must be raised, or the very process of submitting a vote must incur sufficiently high costs. To make this concrete, consider some real-world examples:</p> <ul> <li>When you show up to vote in your local government election, you are asked to present an ID (e.g. a passport) that is (hopefully) expensive to fake. In theory, nothing stops you from generating multiple fake IDs, but if the costs are high enough (monetary costs of making a fake, risk of being caught, etc), than the cost of running a Sybil attack will outweigh its benefits.</li> <li>Alternatively, imagine that you had to incur some other cost (e.g. pay a fee) to submit a vote. If the cost is high enough, then once again, the barrier to running a large-scale Sybil attack is increased.</li> </ul> <p>Note that neither of the above examples "solves" the Sybil attack completely, but they also don't need to: as long as we raise the cost of the attack to be larger than the value gained by successfully subverting the system, then the system is secure and behaves as intended.</p> <div class="callout"> Note that we're using a loose definition of "secure". The system is still open for manipulation, and the exact vote count is affected, but the point is that a malicious participant doesn't affect the final outcome. </div> <h3 id="pow">Proof-of-work as a participation requirement <a href="#pow">#</a> <a class="toc-arrow" href="#toc">&uarr;</a></h3> <p>Any user can easily (and cheaply) generate a new "identity" in our P2P network by generating a new private-public keypair. Similarly, any user can sign a vote with their private key and send it into the P2P network - that's also cheap, as the abundance of spam email in our inboxes clearly illustrates. Hence, submitting new votes is cheap and a malicious user can easily flood the network with as many votes as they wish.</p> <p><strong>However, what if we made one of the steps above expensive such that you had to expend significantly more effort, time, or money?</strong> That's the core idea behind requiring a <a href="https://en.wikipedia.org/wiki/Proof-of-work_system">proof-of-work</a>:</p> <ol> <li>The proof-of-work step should be "expensive" for the sender.</li> <li>The proof-of-work step should be "cheap" to verify by everyone else.</li> </ol> <p>There are many possible implementations of such a method, but for our purposes we can re-use the properties provided by the cryptographic hash functions we encountered earlier:</p> <p><img src='http://www.igvita.com/posts/14/hashfunction.png' class='center' style='max-width:580px;width:100%' /></p> <ol> <li>It is easy to compute the hash value for any given message.</li> <li>It is expensive to generate a message that has a given hash value.</li> </ol> <p><strong>We can impose a new rule in our system requiring that every signed vote must have a hash value that begins with a particular substring - i.e. require a partial hash collision of, say, two zero prefix.</strong> If this seems completely arbitrary, that's because it is - stay with me. Let's walk through the steps to see how this works:</p> <ol> <li>Let's say a valid vote statement is a simple string: <em>"I vote for Bob".</em></li> <li>We can use the same SHA-256 algorithm to <a href="http://www.xorbin.com/tools/sha256-hash-calculator">generate a hash value</a> for our vote. <ul> <li><code>sha256("I vote for Bob") â b28bfa35bcd071a321589fb3a95cac...</code></li> </ul> </li> <li>The resulting hash value is invalid because it does not start with our required substring of two zeros.</li> <li>We modify the vote statement by appending an arbitrary string and try again: <ul> <li><code>sha256("I vote for Bob - hash attempt #2") â 7305f4c1b1e7...</code></li> </ul> </li> <li>The resulting hash value does not satisfy our condition either. We update the value and try again, and again, andâ¦ 155 attempts later we finally get: <ul> <li><code>sha256("I vote for Bob - hash attempt #155") â 008d08b8fe...</code></li> </ul> </li> </ol> <p>The critical property of the above workflow is that the output of the cryptographic hash function (SHA-256 in this case) is completely different every time we modify the input: the hash value of the previous attempt does not tell us anything about what the hash value of the next attempt when we increment our counter. <strong>As a result, generating a valid vote is not just "hard problem", but also one better described as a lottery where each new attempt gives you a random output.</strong> Also, we can adjust the odds of the lottery by changing the length of the required prefix:</p> <ol> <li>Each character of the SHA-256 checksum has 16 possible values: <em>0, 1, 2, 3, 4, 5, 6, 7, 8, 9, a, b, c, d, e, f.</em></li> <li>In order to generate a hash with a valid two zero prefix the sender will need 256 (16<sup>2</sup>) attempts on average.</li> <li>Bumping the requirement to 5 zeros will require more than 1,000,000 (16<sup>5</sup>) attempts on averageâ¦ Point being, we can easily increase the cost and make the sender spend more CPU cycles to find a valid hash.</li> </ol> <div class="callout"> How many SHA256 checksums can we compute on a modern CPU? The cost depends on the size of the message, CPU architecture, and other variables. If you're curious, open up your console and run a benchmark: <code>$> openssl speed sha</code>. </div> <p>The net result is that generating a valid vote is "expensive" for the sender, but is still trivial to verify for the receiver: the receiver hashes the transaction (one operation) and verifies that the checksum contains the required hash collision prefix... Great, so how is this useful for our P2P system? <strong>Above proof-of-work mechanism allows us to adjust the cost of submitting a vote such that the total cost of subverting the system (i.e. spoofing enough valid votes to guarantee a certain outcome) is higher than the value gained by attacking the system.</strong></p> <div class="callout"> Note that the "high cost to generate a message" is a useful property in many other contexts. For example, email spam works precisely because it is incredibly cheap to generate a message. If we could raise the cost of sending an email message - say, by requiring a proof-of-work signature - then we could break the spam business model by raising costs to be higher than profits. </div> <h2 id="mvb">Building the minimum viable block chain <a href="#mvb">#</a> <a class="toc-arrow" href="#toc">&uarr;</a></h2> <p>We've covered a lot of ground. Before we discuss how the block chain can help us build a secure distributed ledger, let's quickly recap the setup, the properties, and the unsolved challenges within of our network:</p> <p><img src='http://www.igvita.com/posts/14/doublespend.png' class='center' style='max-width:690px;width:100%' /></p> <ol> <li>Alice and Bob complete a transaction and record it in their respective ledgers. <ol> <li>Once done, Bob has a <a href="#pki">PKI-protected IOU</a> from Alice.</li> </ol> </li> <li>Bob completes a transaction with John where he <a href="#multiparty">transfers Alice's IOU to John</a>. Both Bob and John update their ledgers, but Alice doesn't know about the transactionâ¦ yet. <ol> <li><strong>Happy scenario:</strong> John asks Alice to redeem his new IOU; Alice verifies his transaction by fetching Bob's public key; if the transaction is valid she pays John the required amount.</li> <li><strong>Not so happy scenario:</strong> Bob uses his old ledger that omits his transaction with John to create a <a href="#doublespend">double-spend transaction</a> with Katy. Next, both Katy and John show up at Alice's doorstep and realize that only one of them will get paid.</li> </ol> </li> </ol> <p>The double-spend is possible due to the "weak consistency" of the distributed ledger: neither Alice nor Katy know about John and Bob's transaction, which allows Bob to exploit this inconsistency to his advantage. Solution? If the network is small and all participants are known, we can require that each transaction must be "accepted" by the network before it is deemed valid:</p> <ul> <li><strong>Unanimous consensus:</strong> whenever a transaction takes place the two parties contact all other participants, tell them about the transaction, and then wait for their "OK" before they commit the transaction. As a result, all of the ledgers are updated simultaneously and double-spend is no longer possible.</li> <li><strong>Quorum consensus:</strong> to improve processing speed and availability of the network (i.e. if someone is offline, we can still process the transaction) we can relax the above condition of unanimous consensus to quorum consensus (50% of the network).</li> </ul> <p>Either of the above strategies would solve our immediate problem for a small network of known and verified participants. However, neither strategy scales to a larger, dynamic network where neither the total number of participants is known at any point in time, nor their identity:</p> <ol> <li>We don't know how many people to contact to get their approval.</li> <li>We don't know whom to contact to get their approval.</li> <li>We don't know whom we are calling.</li> </ol> <div class="callout"> Note that we can use any means of communication to satisfy above worfklow: in person, internet, <a href="http://www.ietf.org/rfc/rfc1149.txt">avian carriers</a>, etc! </div> <p>Lacking identity and global knowledge of all the participants in the network we have to relax our constraints. <strong>While we can't guarantee that any particular transaction is valid, that doesn't stop us from making a statement about the probability of a transaction being accepted as valid:</strong></p> <ul> <li><p><strong>Zero-confirmation transaction:</strong> we can accept a transaction without contacting any other participants. This places full trust on the integrity of the payer of the transaction - i.e. they won't double-spend.</p></li> <li><p><strong>N-confirmation transaction:</strong> we can contact some subset of the (known) participants in the network and get them to verify our transaction. The more peers we contact, the higher the probability that we will catch malicious parties attempting to defraud us.</p></li> </ul> <p><strong>What is a good value for "N"?</strong> The answer depends on the amount being transferred and your trust and relationship with the opposite party. If the amount is small, you may be willing to accept a higher level of risk, or, you may adjust your risk tolerance based on what you know about the other party. Alternatively, you will have to do some extra work to contact other participants to validate your transaction. In either case, there is a tradeoff between the speed with which the transaction is processed (zero-confirmation is instant), the extra work, and the risk of that transaction being invalid.</p> <p><img src='http://www.igvita.com/posts/14/sybil.png' class='left' />So far, so good. Except, there is an additional complication that we must consider: our system relies on transaction confirmations from other peers, but nothing stops a malicious user from generating as many fake identities as needed (recall that an "identity" in our system is simply a public-private keypair, which is trivial to generate) to satisfy Katy's acceptance criteria.</p> <p><strong>Whether Bob decides to execute the attack is a matter of simple economics: if the gain is higher than the cost then he should consider running the attack.</strong> Conversely, if Katy can make the cost of running the attack higher than the value of the transaction, then she should be safe (unless Bob has a personal vendetta and/or is willing to lose money on the transaction... but that's out of scope). To make it concrete, let's assume the following:</p> <ul> <li>Bob is transferring 10 chroms to Katy.</li> <li>The cost of generating a fake identity and transaction response is 0.001 chroms: energy costs to keep the computer running, paying for internet connectivity, etc.</li> </ul> <p>If Katy asks for 1001 confirmations, then it no longer makes (economic) sense for Bob to run the attack. Alternatively, <strong>we could add a proof-of-work requirement for each confirmation and raise the cost for each valid response from 0.001 chroms to 1:</strong> finding a valid hash will take CPU time, which translates to a higher energy bill. As a result, Katy would only need to ask for 11 confirmations to get the same guarantee.</p> <div class="callout"> Note that Katy also incurs some costs while requesting each confirmation: she has to expend effort to send out the requests and then validate the responses. Further, if the cost of generating a confirmation and verifying it is one-to-one, then Katy will incur the same total cost to verify the transaction as its valueâ¦ which, of course, makes no economic sense.<br/><br/> This is why the asymmetry of proof-of-work is critical. Katy incurs low cost to send out requests and validate the responses, but the party generating the confirmation needs to expend significantly more effort to generate a valid response. </div> <p>Great, problem solved, right? Sort of... in the process we've created another economic dilemma. <strong>Our network now incurs a cost to validate each transaction that is of equal or higher value than the transaction itself.</strong> While this acts as an economic deterrent against malicious participants, why would any legitimate participant be willing to incur any costs for someone else? A rational participant simply wouldn't, it doesn't make sense. Doh.</p> <h3 id="blocks">Adding "blocks" & transaction fee incentives <a href="#blocks">#</a> <a class="toc-arrow" href="#toc">&uarr;</a></h3> <p><strong>If participants in the network must incur a cost to validate each other's transactions, then we must provide an economic incentive for them to do so.</strong> In fact, at a minimum we need to offset their costs, because otherwise an "idle" participant (anyone who is not submitting own transactions) would continue accruing costs on behalf of the network &mdash; that wouldn't work. Also a couple of other problems that we need address:</p> <ol> <li><p>If the cost of verifying the transaction is equal to or higher than the value of the transaction itself (to deter malicious participants), than the total transaction value is net-zero, or negative! E.g. Bob transfers 10 chroms to Katy; Katy spends 10 chroms to compensate other peers to validate the transaction; Katy is sad.</p></li> <li><p>How does Katy pay for confirmations? If that's its own transaction then we have a recursive problem.</p></li> </ol> <p><strong>Let's start with the obvious: the transaction fee can't be as high as the value of the transaction itself.</strong> Of course, Katy doesn't have to spend the exact value to confirm the transaction (e.g. she can allocate half the value for confirmations), but then it becomes a question of margins: if the remaining margin (value of transaction - verification fees) is high enough, than there is still incentive for fraud. Instead, ideally we would like to incur the lowest possible transaction fees and still provide a strong deterrent against malicious participants. Solution?</p> <p><strong>We can incentivize participants in the network to confirm transactions by allowing them to pool and confirm multiple transactions at once - i.e. confirm a "block" of transactions.</strong> Doing so would also allow them to aggregate transaction fees, thereby lowering the validation costs for each individual transaction.</p> <p><img src='http://www.igvita.com/posts/14/blockchain-full.png' class='center' style='max-width:685px;width:100%' /></p> <p><strong>A block is simply a collection (one or more) of valid transactions - think of it as the equivalent of a page in a physical ledger.</strong> In turn, each block contains a reference to a previous block (previous page) of transactions, and the full ledger is a linked sequence of blocks. Hence, <strong>block chain</strong>. Consider the example above:</p> <ol> <li>Alice and Bob generate new transactions and announces them to the network.</li> <li>Chris is listening for new transaction notifications, each of which contains a transaction fee that the sender is willing to pay to get it validated and confirmed by the network: <ol> <li>Chris aggregates unconfirmed transactions until he has a direct financial incentive (sum of transaction fees > his cost) to perform the necessary work to validate the pending transactions.</li> <li>Once over the threshold, Chris first validates each pending transaction by checking that none of the inputs are double-spent.</li> <li>Once all transactions are validated Chris adds an extra transaction to the pending list (indicated in green in the diagram above) that transfers the sum of advertised transaction fees to himself.</li> <li>Chris generates a block that contains the list of pending transactions, a reference to the previous block (such that we can traverse the blocks and see the full ledger), and performs the proof-of-work challenge to generate a block hash value that conforms to accepted rules of the network - e.g. partial hash collision of <em>N</em> leading zeros.</li> <li>Finally, once Chris finds a valid block, he distributes it to all other participants.</li> </ol> </li> <li>Both Alice and Bob are listening for new block announcements and look for their transaction in the list: <ol> <li>Alice and Bob verify integrity of the block - i.e. verify proof-of-work and contained transactions.</li> <li>If the block is valid and their transaction is in the list, then the transaction has been confirmed!</li> </ol> </li> </ol> <div class="callout"> We made a big leap here. Previously we've only had one type of record in our network - the signed transaction. Now we have signed transactions and blocks. The former is generated by the individuals engaging in trade, and the latter is generated by parties interested in collecting fees by validating and confirming transactions.<br/><br/> Also, note that the above scheme requires some minimum volume of transactions in the system to sustain the incentives for individuals creating the blocks: the more transactions there are, the lower the fees have to be for any single transaction. </div> <p><strong>Phew, ok, Alice has announced a new transaction and received a valid block from Chris confirming it. That's one confirmation, what about the rest?</strong> Also, Chris is (hopefully) not the only participant who is incentivized to work on generating the blocks. What if someone else generates a different block at the same time, and which of those blocks is "valid"? This is where it gets interesting...</p> <h3 id="incentives">Racing to claim the transaction fees <a href="#incentives">#</a> <a class="toc-arrow" href="#toc">&uarr;</a></h3> <p><strong>The remarkable part about introducing the ability to aggregate fees by verifying a block of transactions is that it creates a role for a new participant in the network who now has a direct financial incentive to secure it.</strong> You can now make a profit by validating transactions, and where there is profit to be made, competition follows, which only strengthens the network - a virtuous cycle and a clever piece of social engineering!</p> <p>That said, the incentive to compete to validate transactions creates another interesting dilemma: how do we coordinate this block generation work in our distributed network? The short answer is, as you may have already guessed, we don't. Let's add some additional rules into our system and examine how they resolve this problem:</p> <ol> <li>Any number of participants is allowed to participate ("race") to create a valid block. There is no coordination. Instead, each interested participant listens for new transactions and decides whether and when they want to try to generate a valid block and claim the transaction fees.</li> <li>When a valid block is generated, it is immediately broadcast into the network. <ol> <li>Other peers check the validity of the block (check each transaction and validity of the block itself), and if valid, add it to their ledgers and then finally rebroadcast it to other peers in the network.</li> <li>Once added, the new block becomes the "topmost block" of their ledger. As a result, if that same peer was also working on generating a block, then they need to abort their previous work and start over: they now need to update their reference to the latest block and also remove any transactions from their unconfirmed list that are contained in the latest block.</li> <li>Once the above steps are complete, they start working on a new block, with the hope that they'll be the first ones to discover the next valid block, which would allow them to claim the transaction fees.</li> </ol> </li> <li>â¦ repeat the above process until the heat death of the universe.</li> </ol> <p>The lack of coordination between all the participants working on generating the blocks means there will be duplicate work in the network, and that's OK! <strong>While no single participant is guaranteed to claim any particular block, as long as the expected value (probability of claiming the block times the expected payout, minus the costs) of participating in the network is positive, then the system is self-sustaining.</strong></p> <div class="callout"> Note that there is also no consensus amongst the peers on which transactions should be validated next. Each participant aggregates their own list and can use different strategies to optimize their expected payoff. Also, due to the nature of our proof-of-work function (finding a partial hash collision for a SHA-256 checksum of the block), the only way to increase the probability of claiming a block is to expend more CPU cycles. </div> <p><img src='http://www.igvita.com/posts/14/blockchain-race.png' class='center' style='max-width:630px;width:100%' /></p> <p>There is one more caveat that we need to deal with: it's possible that two peers will find a valid block at about the same time and begin propagating through the network - e.g. Kent and Chris in the diagram above. As a result, some fraction of the network may end up accepting Kent's block as topmost block, while the rest will take Chris's block. Now what?</p> <h3 id="conflicts">Resolving chain conflicts <a href="#conflicts">#</a> <a class="toc-arrow" href="#toc">&uarr;</a></h3> <p>Once again, we're going to take a hands-off approach and let the random nature of the block generation process resolve the conflict, albeit with one additional rule: if multiple chains are detected, the participants should immediately switch to and build on top of the longest chain. Let's see how this work in practice:</p> <p><img src='http://www.igvita.com/posts/14/blockchain-resolve.png' class='center' style='max-width:675px;width:100%' /></p> <ol> <li>Some peers will start building new blocks on top of Kent's block, others on top of Chris's block.</li> <li>At some point, someone will find a new block and begin propagating it through the network. <ol> <li>When other peers receive the new block, the part of the network that was working with a different topmost block will detect that there is now a longer alternative chain, which means that they need to switch to it - e.g. in the above example, the peers who were working with Chris's block stop their work, drop Chris's block, and switch to the longer (Amy + Kent's) chain.</li> <li>Any transactions that are part of the discarded block but that are not yet confirmed are placed in the pending list and the process starts over.</li> </ol> </li> </ol> <div class="callout">It's possible that the race condition can persist for multiple blocks, but eventually one branch will race ahead of the other and the rest of the network will converge on the same longest chain.</div> <p>Great, we now have a strategy to resolve conflicts between different chains in the network. Specifically, the network promises linearizability of transactions by recording them in a linked list of blocks. But, crucially, it makes no promises about an individual block "guaranteeing" the state of any transaction. Consider the example above:</p> <ul> <li>Alice sends out her transaction into the network.</li> <li>Chris generates a valid block that confirms her transaction.</li> </ul> <p>Except, there is a fork in the chain and Chris's block is later "removed" as the network converges on Kent's branch of the chain. As a result, even when Alice receives a block with her transaction, she can't be sure that this block won't be undone in the future!</p> <h3 id="guarantees">Blocks are never "final" <a href="#guarantees">#</a> <a class="toc-arrow" href="#toc">&uarr;</a></h3> <p>No block is "final", ever. Any block can be "undone" if a longer chain is detected. In practice, forks should be detected fairly quickly, but there is still always the possibility of an alternative chain. Instead, <strong>the only claim we can make is that the "deeper" any particular block is in the chain, the less likely it is that it will undone.</strong> Consequently, no transaction can ever be treated as "final" either, we can only make statements about the probability of it being undone.</p> <ol> <li><strong>0-confirmation transaction:</strong> exchange is performed without waiting for any block to include the transaction.</li> <li><strong>1-confirmation transaction:</strong> latest valid block includes the transaction.</li> <li><strong>N-confirmation transaction:</strong> there is a valid block that includes the transactions, and there are <em>N-1</em> blocks that have since been built on top of that block.</li> </ol> <p>If you are willing to accept the risk, you always have the option to go with a 0-confirmation transaction: no transaction fees, no need to wait for confirmations. However, you also place a lot of trust in the opposite party.</p> <p>Alternatively, if you want to lower your risk, then you should wait for one or more blocks to be built on top of the block that includes your transaction. <strong>The longer you wait, the more blocks will be built on top of the block that contains your transaction, the lower the probability of an alternative chain that may undo your transaction.</strong></p> <div class="callout"> By "undo" we mean any scenario where one of the participants can make the network accept an alternative transaction transferring funds to any account other than yours - e.g. you complete the transaction, hand over the widget and get a receipt, but the attacker then injects a transaction that "double-spends" those same funds to an alternative account. </div> <p>Why does the length of the block chain act as a good proxy for "safety" of a transaction? If an attacker wanted to undo a particular transaction, then they will need to build a chain that begins at a block prior to the one where that transaction is listed, and then build a chain of other blocks that is longer than the one currently used by the network. As a result, the deeper the block, the higher the amount of computational effort that would be required to replace it by creating an alternative chain. The longer the chain the more expensive it is to execute an attack.</p> <div class="callout"> How many blocks should you wait for before accepting a transaction? There is no one number, the answer depends on the properties of the network (time to generate each block, propagation latency of the transactions and blocks, size of the network, etc), and the transaction itself: it's value, what you know about the other party, your risk profile, and so on. </div> <h2 id="properties">Properties of the (minimum viable) block chain<a href="#properties">#</a> <a class="toc-arrow" href="#toc">&uarr;</a></h2> <ol> <li><strong>Individual transactions are secured by PKI.</strong> <ul> <li>Transactions are authenticated: a malicious party can't masquerade as someone else and sign a transaction on their behalf. <ul> <li>Authentication is only with respect to the public-private keypair. There is no requirement for "strong authentication" that links the keypair to any other data about the participants. In fact, a single participant can generate and use multiple keypairs! In this sense, the network allows anonymous transactions.</li> </ul> </li> <li>Non-repudiation: participants can't claim that the transaction did not happen after the fact.</li> <li>Integrity: transactions can't be modified after the fact.</li> </ul> </li> <li><strong>Once created, transactions are broadcast into the P2P network.</strong> <ul> <li>Participants form a network where transactions and blocks are relayed amongst all the participating peers. There no central authority.</li> </ul> </li> <li><p><strong>One or more transactions are aggregated into a "block".</strong></p> <ul> <li>A block validates one or more transactions and claims the transaction fees. <ul> <li>This allow the transaction fees to remain small relative to the value of each transaction.</li> </ul> </li> <li>A valid block must have valid proof-of-work solution. <ul> <li>Valid proof-of-work output is hard to generate and cheap to verify.</li> <li>Proof-of-work is used to raise the cost of generating a valid block to impose a higher cost on running an attack against the network.</li> </ul> </li> <li>Any peer is allowed to work on generating a valid block, and once a valid block is generated, it is broadcast into the network. <ul> <li>Any number of peers can compete to generate a valid block. There is no coordination. When a fork is detected, it is resolved by automatically switching to the longest chain.</li> </ul> </li> <li>Each block contains a link to a previous valid block, allowing us to traverse the full history of all recorded transactions in the network.</li> </ul> </li> <li><p><strong>Peers listen for new block announcements and merge them into their ledgers.</strong></p> <ul> <li>Inclusion of the transaction in a block acts as a "confirmation" of that transaction, but that fact alone does not "finalize" any transaction. Instead, we rely on the length of the chain as a proxy for "safety" of the transaction. Each participant can choose their own level of risk tolerance, ranging from 0-confirmation transactions to waiting for any arbitrary number of blocks.</li> </ul> </li> </ol> <p>The combination of all of the above rules and infrastructure provides a decentralized, peer-to-peer block chain for achieving distributed consensus of ordering of signed transactions. That's a mouthful, I know, but it's also an ingenious solution to a very hard problem. The individual pieces of the block-chain (accounting, cryptography, networking, proof-of-work), are not new, but the emergent properties of the system when all of them are combined are pretty remarkable.</p> <div class="feedflare"> <a href="http://feeds.igvita.com/~ff/igvita?a=d0OcUg0P9pw:tt38ey0HSXk:yIl2AUoC8zA"><img src="http://feeds.feedburner.com/~ff/igvita?d=yIl2AUoC8zA" border="0"></img></a> </div><img src="http://feeds.feedburner.com/~r/igvita/~4/d0OcUg0P9pw" height="1" width="1"/>
</content>
<feedburner:origLink>
http://www.igvita.com/2014/05/05/minimum-viable-block-chain/
</feedburner:origLink>
</entry>
<entry>
<title type="html">Uplink Latency of WiFi and 4G Networks</title>
<link rel="alternate" type="text/html" href="http://feeds.igvita.com/~r/igvita/~3/4jBayzIP-X8/"/>
<updated>2014-04-21T00:00:00-07:00</updated>
<id>
http://www.igvita.com/2014/04/21/uplink-scheduling-in-4G-networks/
</id>
<content type="html">
<p><img src='http://www.igvita.com/posts/14/wifi-4g.png' class='left' style='margin-right:1.5em' /> The user opens your application on their device and triggers an action requiring that we fetch a remote resource: application invokes the appropriate platform API (e.g. XMLHttpRequest), the runtime serializes the request (e.g. translates it to a well-formed HTTP request) and passes the resulting byte buffer to the OS, which then fragments it into one or more TCP packets and finally passes the buffer to the link layer.</p> <p><strong>So far, so good, but what happens next?</strong> As you can guess, the answer depends on the properties of the current link layer in use on the device. Let's dig a bit deeper...</p> <h2>Transmitting over WiFi</h2> <p>If the user is on WiFi, then the link layer breaks up the data into multiple frames and (optimistically) begins transmitting data one frame at a time: it waits until the radio channel is "silent," transmits the WiFi frame, and then waits for an acknowledgement from the receiver before proceeding with transmission of the next frame. Yes, you've read that right, each frame requires a full roundtrip between the sender and receiver! 802.11n is the first standard to introduce "<a href="https://en.wikipedia.org/wiki/Frame_aggregation">frame aggregation</a>," which allows multiple frames to be sent in a single transmission.</p> <p>Of course, not all transmissions will succeed on their first attempt. If two peers transmit at the same time and on the same channel then a collision will happen and both peers will have to retransmit data: both peers sleep for a random interval and then repeat the process. The WiFi access model is <a href="http://chimera.labs.oreilly.com/books/1230000000545/ch06.html">simple to understand</a> and implement, but as you can guess, also doesn't provide any guarantees about the latency costs of the transmission. If the network is mostly idle, then transmission times are nice and low, but if the network is congested, then all bets are off.</p> <p><img src='http://www.igvita.com/posts/14/wifi-latency.png' class='center' style='max-width:695px;width:100%' /></p> <p>In fact, don't be surprised to see 100ms+ delays just for the first hop between the WiFi sender and the access point - e.g. see the histogram above, showing 180ms+ first-hop latency tails on my own (home) WiFi network. That said, note that there is no "typical" or "average" uplink WiFi latency: the latency will depend on the conditions and load of the particular WiFi network. In short, <strong>expect high variability and long latency tails, with an occasional chance of network collapse if too many peers are competing for access.</strong></p> <div class="callout"> If your WiFi access point is also your gateway then you can <a href="https://gist.github.com/igrigorik/4370730">run a simple ping command</a> to measure your first hop latency. </div> <h2>Uplink scheduling on 4G networks</h2> <p>In order to make better use of the limited capacity of the shared radio channel and optimize energy use on the device, <strong>4G/LTE standards take a much more <a href="http://chimera.labs.oreilly.com/books/1230000000545/ch07.html#RRC">hands-on approach</a> to scheduling and resource assignment: the radio tower (eNodeB) notifies the device for when it should listen for inbound data, and also tells the device when it is allowed to transmit data.</strong> As you can imagine, this can incur a lot of coordination overhead (read, latency), but such is the cost of achieving higher channel and energy efficiency.</p> <p><img src='http://www.igvita.com/posts/14/lte-uplink.png' class='center' style='max-width:482px;width:100%' /></p> <ol> <li><p>The radio network has a dedicated <em>Physical Uplink Control Channel (PUCCH)</em> which is used by the device to notify the radio network that it wants to transmit data: each device has a periodic timeslot (typically on a 5, 10, or 20 ms interval) where it is allowed to send a <em>Scheduling Request (SR)</em> that consists of a single bit indicating that it needs uplink access.</p></li> <li><p>The SR request bit is received by the radio tower (eNodeB) but the SR request on its own is not sufficient to assign uplink resources as it doesn't tell the scheduler the amount of data that the device intends to transfer. So, the eNodeB responds with a small "uplink grant" that is just large enough to communicate the size of the pending buffer.</p></li> <li><p>Once the device receives its first uplink grant, it waits for its turn to transmit (up to ~5 ms), and sends a <em>Buffer Status Report (BSR)</em> indicating the amount of application data pending in its upload buffers. Finally, the eNodeB receives the BSR message, allocates the necessary uplink resources and sends back another uplink grant that will allow the device to drain its buffer.</p></li> </ol> <p><strong>What happens if additional data is added to the device buffer while the above process is underway?</strong> Simple, the device sends another BSR message and waits for new uplink grant! If timed correctly, then the BSR requests and uplink grants can be pipelined with existing data transfers from the device, allowing us to minimize first-hop delays. On the other hand, once the device buffer is drained and then new data arrives, the entire process is repeated once over: SR, uplink grant, BSR, uplink grant, data transmission.</p> <p><strong>So, what does this all mean in practice?</strong> Let's do the math:</p> <ul> <li>If the network is configured to use a 10 ms periodic interval for communicating SR messages then we would expect a ~5 ms average delay before the SR request is sent.</li> <li>There are two full roundtrips between the device and the eNodeB to negotiate the uplink resource assignment to transmit pending application data. The latency incurred by these roundtrips will vary for each network, but as a rule of thumb each exchange is ~5 ms.</li> </ul> <p><strong>Add it all up, and we're looking at 20+ ms of delay between application data arriving at the (empty buffer) of the link layer on the device and the same data being available at the link layer of the eNodeB.</strong> From there the packet needs to <a href="http://chimera.labs.oreilly.com/books/1230000000545/ch07.html#_end_to_end_carrier_architecture">traverse the carrier network</a>, exit onto the public network, and get routed to your server.</p> <div class="callout"> Above uplink latency overhead is one reason why low latency applications, such as delivering voice, can be a big challenge over 4G networks. In fact, for voice specifically, there is ongoing work on <a href="https://en.wikipedia.org/wiki/LTE_(telecommunication)#Voice_calls">Voice over LTE</a> (VoLTE) which aims to address this problem. How? Well, one way is to provide a persistent uplink grant: transmit up to X bytes on a Y periodic interval. Believe it or not, today most 4G networks still fall back to old 3G infrastructure to transmit voice! </div> <h2>Optimizing for WiFi and 4G networks</h2> <p>As you can tell, both WiFi and 4G have their challenges. WiFi can deliver low latency first hop if the network is mostly idle: no coordination is required and the device can transmit whenever it senses that the radio channel is idle. On the other hand, WiFi is subject to high variability and long latency tails if the network has many peers competing for access - and most networks do.</p> <p>By contrast, 4G networks require coordination between the device and the radio tower for each uplink transfer, which translates to higher minimum latency, but the upside is that 4G can reign in the latency tails and provides <a href="http://chimera.labs.oreilly.com/books/1230000000545/ch07.html#_real_world_3g_4g_and_wifi_performance">more predictable performance</a> and reduces congestion.</p> <p><strong>So, how does all this impact application developers?</strong> First off, latency aside, and regardless of wireless technology, consider the energy costs of your network transfers! Periodic transfers <a href="http://chimera.labs.oreilly.com/books/1230000000545/ch08.html#ELIMINATE_POLLING">incur high energy overhead</a> due to the need to wake up the radio on each transmission. Second, same periodic transfers also incur high uplink coordination overhead - 4G in particular. <strong>In short, don't trickle data.</strong> Aggregate your network requests and fire them in one batch: you will reduce energy costs and reduce latency by amortizing scheduling overhead.</p> <div class="feedflare"> <a href="http://feeds.igvita.com/~ff/igvita?a=4jBayzIP-X8:Xv6TfxI4Kac:yIl2AUoC8zA"><img src="http://feeds.feedburner.com/~ff/igvita?d=yIl2AUoC8zA" border="0"></img></a> </div><img src="http://feeds.feedburner.com/~r/igvita/~4/4jBayzIP-X8" height="1" width="1"/>
</content>
<feedburner:origLink>
http://www.igvita.com/2014/04/21/uplink-scheduling-in-4G-networks/
</feedburner:origLink>
</entry>
<entry>
<title type="html">Why is my CDN 'slow' for mobile clients?</title>
<link rel="alternate" type="text/html" href="http://feeds.igvita.com/~r/igvita/~3/dvWWcqJYd9c/"/>
<updated>2014-03-26T00:00:00-07:00</updated>
<id>
http://www.igvita.com/2014/03/26/why-is-my-cdn-slow-for-mobile-clients/
</id>
<content type="html">
<p><img src='http://www.igvita.com/posts/14/radio-tower.png' class='left' style='margin-right:1.5em' /><blockquote>We're using a CDN but when we look at the performance numbers it appears to be far less effective for mobile clients. We're considering disabling it entirely as we're not sure that it's worth it. Someone needs to build a special CDN for mobile, we'd definitely use that to improve latency!</blockquote></p> <p>The frequency with which I've been hearing this and similar arguments has been rapidly increasing as more teams are finally focusing on improving performance of their mobile sites. <strong>The problem is, while the statement is often based on real data (i.e. the relative performance improvements offered by a CDN are smaller for mobile clients), the conclusion is wrong: the absolute improvements are likely the same for all clients and hence worth every penny</strong>. Also, we don't need a "mobile CDN", we need carriers to fix their networks.</p> <h2>The many components of latency</h2> <p>To understand why the relative metrics between desktop and mobile CDN performance are misleading many otherwise well-informed teams, it helps to take a step back and consider the actual topology of the internet, plus what CDNs can and cannot do. In fact, for sake of a concrete example, let's assume the following:</p> <ul> <li>Client is located on the west coast; server is located on the east coast.</li> <li>The propagation latency between west and east US coasts is 50 ms.</li> <li>The server response time is 50 ms.</li> <li>Last-mile latency for "tethered" clients: ~18 ms for fiber, ~26 ms for cable, ~44 ms for DSL.</li> <li>Last-mile latency for "wireless" clients: ~50 ms for 4G, ~200 ms for 3G.</li> </ul> <p><img src='http://www.igvita.com/posts/14/last-mile-latency.png' class='center' style='max-width:516px;width:100%' /></p> <p>The total time to service the request is a combination of all of the latencies in both directions: last-mile &rarr; coast to coast (50 ms) &rarr; server response (50 ms) &rarr; coast to coast (50 ms) &rarr; last-mile. For example, if the client happens to be on a fiber connection with a 18 ms last-mile path, then the total time is 18+50+50+50+18, or 186 milliseconds.</p> <div class="callout"> The last-mile latencies we're using above are specific to the US and are courtesy of the <a href="http://www.fcc.gov/measuring-broadband-america/2013/February">Measuring Broadband America report</a> (FCC, 2013). Sadly, FCC has not yet published any similar reports for US mobile carriers. As a result, the ~50 and ~200ms numbers are based on <a href="http://chimera.labs.oreilly.com/books/1230000000545/ch07.html#MOBILE_JITTER">specified and cited targets</a> of existing US carriers. </div> <h2>Accelerating content delivery with a CDN</h2> <p>The whole premise of a CDN is to move the bytes as close to the user as possible and to do so CDNs deploy cache servers within various data centers and <a href="https://www.peeringdb.com/">peering points</a> around the globe. In other words, <strong>in the optimal case the CDN servers are located immediately outside the ISP / carrier network: the client makes a request, incurs the cost of last-mile latency to exit the ISP / carrier network and immediately hits a CDN server that returns a response.</strong> As a result, CDN minimizes the propagation latency and if a static resource is served, also reduces the server response time by returning a cached asset.</p> <p>To continue with our earlier example, let's assume that our CDN server is optimally placed (~5 ms path instead of 50 ms coast-to-coast path) and that we are requesting a publicly cacheable asset that can be served by the CDN (~5ms) without hitting the origin. For our fiber client, the new total time is the sum of the last-mile roundtrip plus the CDN response times: 18+5+5+5+18, or 51 ms in total. As a result, adding a CDN took our request time from ~186ms down to ~51ms: a 365% improvement in total latency!</p> <table> <tbody> <tr> <td class="header"></td> <td class="header">Last-mile</td> <td class="header">Coast-to-Coast</td> <td class="header">Server Response</td> <td class="header">Total (ms)</td> <td class="header">Improvement</td> </tr> <tr> <td class="header">Fiber</td> <td>18</td> <td>50</td> <td>50</td> <td>186</td> <td></td> </tr> <tr> <td class="header">Cable</td> <td>26</td> <td>50</td> <td>50</td> <td>202</td> <td></td> </tr> <tr> <td class="header">DSL</td> <td>44</td> <td>50</td> <td>50</td> <td>238</td> <td></td> </tr> <tr> <td class="header">4G</td> <td>50</td> <td>50</td> <td>50</td> <td>250</td> <td></td> </tr> <tr> <td class="header">3G</td> <td>200</td> <td>50</td> <td>50</td> <td>550</td> <td></td> </tr> <tr> <td colspan="6" style="background-color:#ccc"></td> </tr> <tr> <td class="header">CDN + Fiber</td> <td>18</td> <td>5</td> <td>5</td> <td>51</td> <td style="background-color: #d9ead3">-135 ms (365%)</td> </tr> <tr> <td class="header">CDN + Cable</td> <td>26</td> <td>5</td> <td>5</td> <td>67</td> <td style="background-color: #d9ead3">-135 ms (301%)</td> </tr> <tr> <td class="header">CDN + DSL</td> <td>44</td> <td>5</td> <td>5</td> <td>103</td> <td style="background-color: #fff2cc">-135 ms (231%)</td> </tr> <tr> <td class="header">CDN + 4G</td> <td>50</td> <td>5</td> <td>5</td> <td>115</td> <td style="background-color: #fff2cc">-135 ms (217%)</td> </tr> <tr> <td class="header">CDN + 3G</td> <td>200</td> <td>5</td> <td>5</td> <td>415</td> <td style="background-color: #f4cccc">-135 ms (133%)</td> </tr> </tbody> </table> <p>Repeating the same calculation for each connection profile reveals an unfortunate trend: <strong>the relative effectiveness of the CDN "declines" as the last-mile latency increases. If you consider the topology of where the CDN servers are typically placed this makes perfect sense: CDN servers are outside the ISP network. That said, note that the absolute latency improvement is still the same regardless of the last-mile profile.</strong></p> <p>CDNs help minimize propagation and server response times. If your only metric is the relative before and after improvement, then it would appear that a CDN is not doing much for mobile clients - e.g. a mere 33% improvement for 3G clients. In reality, the savings are the same, and the real takeaway is that the last-mile latency of most mobile carriers dominates all other latencies of the resource transfer - yes, a rather sad state of affairs.</p> <h2>Operational and business costs of living at the edge</h2> <p>One obvious strategy to improve the end-to-end latency is to move the cache servers even closer to the client: instead of positioning them outside the ISPs network, could we move them inside? In principle, the answer is yes, and many ISPs already deploy their own cache servers. However, in practice, this is a tricky problem.</p> <p>First, the number of peering points is <a href="https://www.peeringdb.com/">relatively small</a>, which allows CDNs to deploy in dozens of well known locations around the globe to provide their service. Also, to do so, they don't have to do any special deals with individual ISPs: typically, the servers are deployed in shared data centers (peering points). By contrast, moving servers into the ISP network would require individual deals with each ISP - not impossible, but obviously a much harder operational and business problem.</p> <p>For the sake of argument, let's say a CDN does strike a deal with an ISP. Now the CDN needs to deploy many more servers, and ideally as close to their customers as possible (near the radio towers and other aggregation points). Doing so would require a lot of hardware, makes maintenance and upgrades an operations nightmare, and opens a lot of security questions - e.g. would you deploy a TLS termination node within a third-party operated network that you don't have direct access to? In short, it's a cost, security, and a logistics nightmare.</p> <p>That's not to say this can't be done. Many ISPs have long been trying to move "upmarket" and provide CDN functionality. However, ISPs have a different problem: it's very hard for them to sign clients because most sites are not interested in signing individual deals with every ISP on the planet. In recent news, <a href="http://newscenter.verizon.com/corporate/news-articles/2013/12-09-verizon-plans-to-acquire-edgecast-networks/">Verizon acquired EdgeCast</a>... It remains to be seen what they do with it and if this will actually benefit Verizon clients.</p> <p><strong>Business and operational costs aside, there is nothing particularly special about optimizing CDNs for mobile clients.</strong> The root problem is that the last-mile latency of mobile carriers is atrocious - that's what we need to fix. Instead of pushing cache servers closer to the edge, we need transparency into performance of these networks, and we need more competition between carriers to address the underlying last-mile performance problems.</p> <h2>CDNs are not slow for mobile, use them</h2> <p>In short, there is no reason why CDNs are inherently 'slower' for mobile clients: don't confuse relative gains with absolute savings. That said, the actual performance will obviously vary for each CDN provider based on location of their servers and connectivity with various mobile carriers - measure, gather real data, optimize.</p> <div class="feedflare"> <a href="http://feeds.igvita.com/~ff/igvita?a=dvWWcqJYd9c:u_itgaFWZKQ:yIl2AUoC8zA"><img src="http://feeds.feedburner.com/~ff/igvita?d=yIl2AUoC8zA" border="0"></img></a> </div><img src="http://feeds.feedburner.com/~r/igvita/~4/dvWWcqJYd9c" height="1" width="1"/>
</content>
<feedburner:origLink>
http://www.igvita.com/2014/03/26/why-is-my-cdn-slow-for-mobile-clients/
</feedburner:origLink>
</entry>
<entry>
<title type="html">Optimizing Web Font Rendering Performance</title>
<link rel="alternate" type="text/html" href="http://feeds.igvita.com/~r/igvita/~3/jOIw8pX2ELM/"/>
<updated>2014-01-31T00:00:00-08:00</updated>
<id>
http://www.igvita.com/2014/01/31/optimizing-web-font-rendering-performance/
</id>
<content type="html">
<p><img src='http://www.igvita.com/posts/14/font-measurements.png' class='left' />Web font adoption continues to accelerate across the web: according to HTTP Archive, <a href="http://httparchive.org/trends.php#perFonts">~37% of top 300K sites are using web fonts</a> as of early 2014, which translates to a <em>2x+</em> increase over the past twelve months. Of course, this should not be all that surprising to most of us. <strong>Typography has always been an important part of good design, branding, and readability and web fonts offer many additional benefits: the text is selectable, searchable, zoomable, and high-DPI friendly.</strong> What's not to like?</p> <p>Ah, but what about the rendering speed, don't web fonts come with a performance penalty? Fonts are an additional critical resource on the page, so yes, they can impact rendering speed of our pages. That said, <strong>just because the page is using web fonts doesn't mean it will (or has to) render slower.</strong></p> <p>There are four primary levers that determine the performance impact of web fonts on the page:</p> <ol> <li><strong>The total number of fonts and font-weights used on the page.</strong></li> <li><strong>The total byte size of fonts used on the page.</strong></li> <li><strong>The transfer latency of the font resource.</strong></li> <li><strong>The time when the font downloads are initiated.</strong></li> </ol> <p>The first two levers are directly within the control of the designer of the page. The more fonts are used, the more requests will be made and more bytes will be incurred. The general UX best practice is to keep the number of used fonts at a minimum, which also aligns with our performance goals. Step one: use web fonts, but audit your font usage periodically and try to keep it lean.</p> <h2><a name="resource-timing"></a> Measuring web font latencies</h2> <p>The transfer latency of each font file is dependent on its bytesize, which in turn is determined by the number of glyphs, font metadata (e.g <a href="http://en.wikipedia.org/wiki/Font_hinting">hinting</a> for Windows platforms), and used compression method. Techniques such as font subsetting, <a href="http://www.igvita.com/2012/09/12/web-fonts-performance-making-pretty-fast/#optimizing">UA-specific optimization</a>, and more efficient compression (e.g. Google Fonts recently <a href="https://plus.google.com/+IlyaGrigorik/posts/1sxencNkbNS">switched to Zopfli</a> for WOFF resources), are all key to optimizing the transfer size. Plus, since we're talking about latency, where the font is served from makes a difference also &ndash; i.e. a CDN, and ideally the user's cache!</p> <p>That said, instead of talking in the abstract, <strong>how long does it actually take the visitor to download the web font resource on your site?</strong> The best way to answer this question is to instrument your site via the Resource Timing API, which allows us to get the DNS, TCP, and transfer time data for each font - as a bonus, Google Fonts <a href="http://googledevelopers.blogspot.com/2013/12/measuring-network-performance-with.html">recently enabled Resource Timing</a> support! Here is an example snippet to report font latencies to Google Analytics:</p> <div class="highlight"><pre><code class="javascript"><span class="c1">// check if visitor&#39;s browser supports Resource Timing</span> <span class="k">if</span> <span class="p">(</span><span class="k">typeof</span> <span class="nb">window</span><span class="p">.</span><span class="nx">performance</span> <span class="o">==</span> <span class="s1">&#39;object&#39;</span><span class="p">)</span> <span class="p">{</span> <span class="k">if</span> <span class="p">(</span><span class="k">typeof</span> <span class="nb">window</span><span class="p">.</span><span class="nx">performance</span><span class="p">.</span><span class="nx">getEntriesByName</span> <span class="o">==</span> <span class="s1">&#39;function&#39;</span><span class="p">)</span> <span class="p">{</span> <span class="kd">function</span> <span class="nx">logData</span><span class="p">(</span><span class="nx">name</span><span class="p">,</span> <span class="nx">r</span><span class="p">)</span> <span class="p">{</span> <span class="kd">var</span> <span class="nx">dns</span> <span class="o">=</span> <span class="nb">Math</span><span class="p">.</span><span class="nx">round</span><span class="p">(</span><span class="nx">r</span><span class="p">.</span><span class="nx">domainLookupEnd</span> <span class="o">-</span> <span class="nx">r</span><span class="p">.</span><span class="nx">domainLookupStart</span><span class="p">),</span> <span class="nx">tcp</span> <span class="o">=</span> <span class="nb">Math</span><span class="p">.</span><span class="nx">round</span><span class="p">(</span><span class="nx">r</span><span class="p">.</span><span class="nx">connectEnd</span> <span class="o">-</span> <span class="nx">r</span><span class="p">.</span><span class="nx">connectStart</span><span class="p">),</span> <span class="nx">total</span> <span class="o">=</span> <span class="nb">Math</span><span class="p">.</span><span class="nx">round</span><span class="p">(</span><span class="nx">r</span><span class="p">.</span><span class="nx">responseEnd</span> <span class="o">-</span> <span class="nx">r</span><span class="p">.</span><span class="nx">startTime</span><span class="p">);</span> <span class="nx">_gaq</span><span class="p">.</span><span class="nx">push</span><span class="p">(</span> <span class="p">[</span><span class="s1">&#39;_trackTiming&#39;</span><span class="p">,</span> <span class="nx">name</span><span class="p">,</span> <span class="s1">&#39;dns&#39;</span><span class="p">,</span> <span class="nx">dns</span><span class="p">],</span> <span class="p">[</span><span class="s1">&#39;_trackTiming&#39;</span><span class="p">,</span> <span class="nx">name</span><span class="p">,</span> <span class="s1">&#39;tcp&#39;</span><span class="p">,</span> <span class="nx">tcp</span><span class="p">],</span> <span class="p">[</span><span class="s1">&#39;_trackTiming&#39;</span><span class="p">,</span> <span class="nx">name</span><span class="p">,</span> <span class="s1">&#39;total&#39;</span><span class="p">,</span> <span class="nx">total</span><span class="p">]</span> <span class="p">);</span> <span class="p">}</span> <span class="kd">var</span> <span class="nx">_gaq</span> <span class="o">=</span> <span class="nx">_gaq</span> <span class="o">||</span> <span class="p">[];</span> <span class="kd">var</span> <span class="nx">resources</span> <span class="o">=</span> <span class="nb">window</span><span class="p">.</span><span class="nx">performance</span><span class="p">.</span><span class="nx">getEntriesByType</span><span class="p">(</span><span class="s2">&quot;resource&quot;</span><span class="p">);</span> <span class="k">for</span> <span class="p">(</span><span class="kd">var</span> <span class="nx">i</span> <span class="k">in</span> <span class="nx">resources</span><span class="p">)</span> <span class="p">{</span> <span class="k">if</span> <span class="p">(</span><span class="nx">resources</span><span class="p">[</span><span class="nx">i</span><span class="p">].</span><span class="nx">name</span><span class="p">.</span><span class="nx">indexOf</span><span class="p">(</span><span class="s2">&quot;themes.googleusercontent.com&quot;</span><span class="p">)</span> <span class="o">!=</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span> <span class="p">{</span> <span class="nx">logData</span><span class="p">(</span><span class="s2">&quot;webfont-font&quot;</span><span class="p">,</span> <span class="nx">resources</span><span class="p">[</span><span class="nx">i</span><span class="p">])</span> <span class="p">}</span> <span class="k">if</span> <span class="p">(</span><span class="nx">resources</span><span class="p">[</span><span class="nx">i</span><span class="p">].</span><span class="nx">name</span><span class="p">.</span><span class="nx">indexOf</span><span class="p">(</span><span class="s2">&quot;fonts.googleapis.com&quot;</span><span class="p">)</span> <span class="o">!=</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span> <span class="p">{</span> <span class="nx">logData</span><span class="p">(</span><span class="s2">&quot;webfont-css&quot;</span><span class="p">,</span> <span class="nx">resources</span><span class="p">[</span><span class="nx">i</span><span class="p">])</span> <span class="p">}</span> <span class="p">}</span> <span class="p">}</span> <span class="p">}</span> </code></pre></div> <p>The above example captures the key latency metrics both for the UA-optimized CSS file and the font files specified in that file: the CSS lives on <code>fonts.googleapis.com</code> and is cached for 24 hours, and font files live on <code>themes.googleusercontent.com</code> and have a long-lived expiry. With that in place, let's take a look at the total <code>(responseEnd - startTime)</code> timing data in Google Analytics for my site:</p> <p><img src='http://www.igvita.com/posts/14/font-stats.png' class='center' style='max-width:729px;width:100%' /></p> <p>For privacy reasons, the Resource Timing API intentionally does not provide a "fetched from cacheâ indicator, but we can nonetheless use a reasonable timing threshold - say, 20ms - to get an approximation. Why 20ms? Fetching a file from spinning rust, and even flash, is <a href="http://www.igvita.com/2009/06/23/measuring-optimizing-io-performance/">not free</a>. The actual cache-fetch timing will vary based on hardware, but for our purposes we'll go with a relatively aggressive 20ms threshold.</p> <p>With that in mind and based on above data for visitors coming to my site, the median time to get the CSS file is ~100ms, and ~26% of visitors get it from their local cache. Following that, we need to fetch the required font file(s), which take &lt;20ms at the median &ndash; a significant portion of the visitors has them in their browser cache! This is great news, and a confirmation that the <a href="http://www.igvita.com/2012/09/12/web-fonts-performance-making-pretty-fast/#serving">Google Fonts strategy of long-lived and shared font resources</a> is working.</p> <p>Your results will vary based on the fonts used, amount and type of traffic, plus other variables. The point is that <strong>we don't have to argue in the abstract about the latency and performance costs of web fonts: we have the tools and APIs to measure the incurred latencies precisely.</strong> And what we can measure, we can optimize.</p> <h2><a name="timeouts"></a> Timing out slow font downloads</h2> <p>Despite our best attempts to optimize delivery of font resources, sometimes the user may simply have a poor connection due to a congested link, poor reception, or a variety of other factors. In this instance, the critical resources &ndash; including font downloads &ndash; may block rendering of the page, which only makes the matter worse. To deal with this, and specifically for web fonts, different browsers have taken different routes:</p> <ul> <li>IE immediately renders text with the fallback font and re-renders it once the font download is complete.</li> <li>Firefox holds font rendering for up to 3 seconds, after which it uses a fallback font, and once the font download has finished it re-renders the text once more with the downloaded font.</li> <li><strong>Chrome and Safari hold font rendering until the font download is complete.</strong></li> </ul> <p>There are many good arguments for and against each strategy and we won't go into that discussion here. That said, I think most will agree that the lack of <em>any</em> timeout in Chrome and Safari is not a great approach, and this is something that the Chrome team has been investigating for a while. What should the timeout value be? To answer this, we've instrumented Chrome to gather font-size and fetch times, which yielded the following results:</p> <table cellpadding="0" cellspacing="0"> <tbody> <tr class="header"> <td>Webfont size range</td> <td>Percent</td> <td>50th</td> <td>70th</td> <td>90th</td> <td>95th</td> <td>99th</td> </tr> <tr class="c3"> <td class="size">0KB - 10KB</td> <td class="percent">5.47%</td> <td>136 ms</td> <td>264 ms</td> <td>785 ms</td> <td>1.44 s</td> <td class="orange">5.05 s</td> </tr> <tr> <td class="size">10KB - 50KB</td> <td class="percent">77.55%</td> <td>111 ms</td> <td>259 ms</td> <td>892 ms</td> <td>1.69 s</td> <td class="orange">6.43 s</td> </tr> <tr class="c3"> <td class="size">50KB - 100KB</td> <td class="percent">14.00%</td> <td>167 ms</td> <td>882 ms</td> <td>1.31 s</td> <td>2.54 s</td> <td class="orange">9.74 s</td> </tr> <tr class="c3"> <td class="size">100KB - 1MB</td> <td class="percent">2.96%</td> <td>198 ms</td> <td>534 ms</td> <td>2.06 s</td> <td class="orange">4.19 s</td> <td class="red">10+ s</td> </tr> <tr class="c3"> <td class="size">1MB+</td> <td class="percent">0.02%</td> <td>370 ms</td> <td>969 ms</td> <td class="orange">4.22 s</td> <td class="orange">9.21 s</td> <td class="red">10+ s</td> </tr> </tbody> </table> <p>First, the good news is that the majority of web fonts are relatively small (&lt;50KB). Second, most font downloads complete within several hundred milliseconds: picking a 10 second timeout would impact ~0.3% of font requests, and a 3 second timeout would raise that to ~1.1%. Based on this data, <strong>the <a href="https://code.google.com/p/chromium/issues/detail?id=235303#c17">conclusion</a> was to make Chrome mirror the Firefox behavior: timeout after 3 seconds and use a fallback font, and re-render text once the font download has completed.</strong> This behavior will ship in Chrome M35, and I hope Safari will follow.</p> <h2><a name="crp"></a> Hands-on: initiating font resource requests</h2> <p>We've covered how to measure the fetch latency of each resource, but there is one more variable that is often omitted and forgotten: <strong>we also need optimize when the fetch is initiated.</strong> This may seem obvious on the surface, except that it can be a tricky challenge for web fonts in particular. Let's take a look at a hands-on example:</p> <div class="highlight"><pre><code class="css"><span class="k">@font-face</span> <span class="p">{</span> <span class="nt">font-family</span><span class="o">:</span> <span class="s1">&#39;FontB&#39;</span><span class="o">;</span> <span class="nt">src</span><span class="o">:</span> <span class="nt">local</span><span class="o">(</span><span class="s1">&#39;FontB&#39;</span><span class="o">),</span> <span class="nt">url</span><span class="o">(</span><span class="s1">&#39;http://mysite.com/fonts/fontB.woff&#39;</span><span class="o">)</span> <span class="nt">format</span><span class="o">(</span><span class="s1">&#39;woff&#39;</span><span class="o">);</span> <span class="p">}</span> <span class="nt">p</span> <span class="p">{</span> <span class="k">font-family</span><span class="o">:</span> <span class="n">FontA</span> <span class="p">}</span> </code></pre></div> <div class="highlight"><pre><code class="html"><span class="cp">&lt;!DOCTYPE html&gt;</span> <span class="nt">&lt;html&gt;</span> <span class="nt">&lt;head&gt;</span> <span class="nt">&lt;link</span> <span class="na">href=</span><span class="s">&#39;stylesheet.css&#39;</span> <span class="na">rel=</span><span class="s">&#39;stylesheet&#39;</span><span class="nt">&gt;</span> <span class="c">&lt;!-- see content above --&gt;</span> <span class="nt">&lt;style&gt;</span> <span class="k">@font-face</span> <span class="p">{</span> <span class="nt">font-family</span><span class="o">:</span> <span class="s1">&#39;FontA&#39;</span><span class="o">;</span> <span class="nt">src</span><span class="o">:</span> <span class="nt">local</span><span class="o">(</span><span class="s1">&#39;FontA&#39;</span><span class="o">),</span> <span class="nt">url</span><span class="o">(</span><span class="s1">&#39;http://mysite.com/fonts/fontA.woff&#39;</span><span class="o">)</span> <span class="nt">format</span><span class="o">(</span><span class="s1">&#39;woff&#39;</span><span class="o">);</span> <span class="p">}</span> <span class="nt">&lt;/style&gt;</span> <span class="nt">&lt;script </span><span class="na">src=</span><span class="s">&#39;application.js&#39;</span> <span class="nt">/&gt;</span> <span class="nt">&lt;/head&gt;</span> <span class="nt">&lt;body&gt;</span> <span class="nt">&lt;p&gt;</span>Hello world!<span class="nt">&lt;/p&gt;</span> <span class="nt">&lt;/body&gt;</span> <span class="nt">&lt;/html&gt;</span> </code></pre></div> <p>There is a lot going on above: we have an external CSS and JavaScript file, and inline CSS block, and two font declarations. <strong>Question: when will the font requests be triggered by the browser?</strong> Let's take it step by step:</p> <ol> <li>Document parser discovers external <code>stylesheet.css</code> and a request is dispatched.</li> <li>Document parser processes the inline CSS block which declares <code>FontA</code> - we're being clever here, we want the font request to go out as early as possible. Except, it doesn't. More on that in a second.</li> <li>Document parser blocks on external script: we can't proceed until that's fetched and executed.</li> <li>Once the script is fetched and executed we finish constructing the DOM, style calculation and layout is performed, and we finally dispatch request for <code>fontA</code>. At this point, we can also perform the first paint, but we can't render the text with our intended font since the font request is inflight... doh.</li> </ol> <p>The key observation in the above sequence is that font requests are not initiated until the browser knows that the font is actually required to render some content on the page - e.g. we never request <code>FontB</code> since there is no content that uses it in above example! On one hand, this is great since it minimizes the number of downloads. On the other, it also means that <strong>the browser can't initiate the font request until it has both the DOM and the CSSOM and is able to resolve which fonts are required for the current page.</strong></p> <p>In the above example, our external JavaScript blocks DOM construction until it is fetched and executed, which also delays the font download. To fix this, we have a few options at our disposal: (a) eliminate the JavaScript, (b) add an async attribute (if possible), or (c) move it to the bottom of the page. However, the more general takeaway is that font downloads won't start until the browser can compute the the render tree. <strong>To make fonts render faster we need to <a href="http://www.youtube.com/watch?v=I4vX-twze9I">optimize the critical rendering path</a> of the page.</strong></p> <div class="callout"> Tip: in addition to measuring the relative request latencies for each resource, we can also measure and analyze the request start time with Resource Timing! Tracking this timestamp will allow us to determine when the font request is initiated. </div> <h2><a name="layout"></a> Optimizing font fetching in Chrome M33</h2> <p>Chrome M33 landed an important optimization that will significantly improve font rendering performance. The easiest way to explain the optimization is to look at a pre-M33 example timeline that illustrates the problem:</p> <p><img src='http://www.igvita.com/posts/14/layout-fix.png' class='center' style='max-width:760px;width:100%' /></p> <ol> <li>Style calculation completed at ~840ms into the lifecycle of the page.</li> <li>Layout is triggered at ~1040ms, and font request is dispatched immediately after.</li> </ol> <p>Except, why did we wait for layout if we already resolved the styles two hundred milliseconds earlier? Once we know the styles we can figure out which fonts we'll need and immediately initiate the appropriate requests &ndash; that's the new behavior in Chrome M33! On the surface, this optimization may not seem like much, but based on our Chrome instrumentation the gap between style and layout is actually much larger than one would think:</p> <table cellpadding="0" cellspacing="0"> <tbody> <tr class="header"> <td>Percentile</td> <td>50th</td> <td>60th</td> <td>70th</td> <td>80th</td> <td>90th</td> </tr> <tr> <td class="header">Time from Style â Layout</td> <td>132 ms</td> <td>182 ms</td> <td>259 ms</td> <td>410 ms</td> <td>820 ms</td> </tr> </tbody> </table> <p><strong>By dispatching the font requests immediately after first style calculation the font download will be initiated ~130ms earlier at the median and ~800ms earlier at 90th percentile!</strong> Cross-referencing these savings with the font fetch latencies we saw earlier shows that in many cases this will allow us to fetch the font <em>before</em> the layout is done, which means that we won't have to block text rendering at all &ndash; this is a huge performance win.</p> <div class="callout"> Of course, one also should ask the obvious question... Why is the gap between style calculation and layout so large? The first place to start is in Chrome DevTools: capture a timeline trace and check for slow operations (e.g. long-running JavaScript, etc). Then, if you're feeling adventurous, head to <code>chrome://tracing</code> to take a peek under the hood &ndash; it may well be that the browser is simply busy processing and laying out the page. </div> <h2><a name="font-load-events"></a> Optimizing web fonts with Font Load Events API</h2> <p>Finally, we come to the most exciting part of this entire story: <a href="http://dev.w3.org/csswg/css-font-load-events/">Font Load Events API</a>. In a nutshell, this API will allow us to manage and define how and when the fonts are loaded &ndash; <strong>we can schedule font downloads at will, we can specify how and when the font will be rendered, and more.</strong> If you're familiar with the <a href="https://github.com/typekit/webfontloader">Web Font Loader</a> JS library, then think of this API as that and more but implemented natively in the browser:</p> <div class="highlight"><pre><code class="javascript"><span class="kd">var</span> <span class="nx">font</span> <span class="o">=</span> <span class="k">new</span> <span class="nx">FontFace</span><span class="p">(</span><span class="s2">&quot;FontA&quot;</span><span class="p">,</span> <span class="s2">&quot;url(http://mysite.com/fonts/fontA.woff)&quot;</span><span class="p">,</span> <span class="p">{});</span> <span class="nx">font</span><span class="p">.</span><span class="nx">ready</span><span class="p">().</span><span class="nx">then</span><span class="p">(</span><span class="kd">function</span><span class="p">()</span> <span class="p">{</span> <span class="c1">// font loaded.. swap in the text / define own behavior.</span> <span class="p">});</span> <span class="nx">font</span><span class="p">.</span><span class="nx">load</span><span class="p">();</span> <span class="c1">// initiate immediate fetch / don&#39;t block on render tree!</span> </code></pre></div> <p>Font Load Events API gives us complete control over which fonts are used, when they are swapped in (i.e. should they block rendering), and when they're downloaded. In the example above we construct a <code>FontFace</code> object directly in JavaScript and trigger an immediate fetch &ndash; we can inline this snippet at the top of our page and avoid blocking on CSSOM and DOM entirely! <strong>Best of all, you can already play with this API in Canary builds of Chrome, and if all goes well it should find its way into stable release by M35.</strong></p> <h2>Web font performance checklist</h2> <p>Web fonts offer a lot of benefits: improved readability, accessibility (searchable, selectable, zoomable), branding, and when done well, <a href="http://hellohappy.org/beautiful-web-type/">beautiful results</a>. <strong>It's not a question of if web fonts should be used, but how to optimize their use.</strong> To that end, a quick performance checklist:</p> <ol> <li>Audit your font usage and keep it lean.</li> <li>Make sure font resources are optimized - see <a href="http://www.igvita.com/2012/09/12/web-fonts-performance-making-pretty-fast/#optimizing">Google Web Fonts tricks</a>.</li> <li>Instrument your font resources with Resource Timing: measure â optimize.</li> <li>Optimize the transfer latency and time of initial fetch for each font.</li> <li>Optimize your critical rendering path, eliminate unnecessary JS, etc.</li> <li>Spend some time playing with the Font Load Events API.</li> </ol> <p>Just because the page is using a web font, or several, doesn't mean it will (or has to) render slower. A well optimized site can deliver a better and faster experience by using web fonts.</p> <div class="feedflare"> <a href="http://feeds.igvita.com/~ff/igvita?a=jOIw8pX2ELM:493gfDoAE2E:yIl2AUoC8zA"><img src="http://feeds.feedburner.com/~ff/igvita?d=yIl2AUoC8zA" border="0"></img></a> </div><img src="http://feeds.feedburner.com/~r/igvita/~4/jOIw8pX2ELM" height="1" width="1"/>
</content>
<feedburner:origLink>
http://www.igvita.com/2014/01/31/optimizing-web-font-rendering-performance/
</feedburner:origLink>
</entry>
<entry>
<title type="html">Optimizing NGINX TLS Time To First Byte (TTTFB)</title>
<link rel="alternate" type="text/html" href="http://feeds.igvita.com/~r/igvita/~3/5S-L5Oc1hBg/"/>
<updated>2013-12-16T00:00:00-08:00</updated>
<id>
http://www.igvita.com/2013/12/16/optimizing-nginx-tls-time-to-first-byte/
</id>
<content type="html">
<p><img src='http://www.igvita.com/posts/13/nginx-tttfb.png' class='left' />Network latency is one of our <a href="http://www.igvita.com/2012/07/19/latency-the-new-web-performance-bottleneck/">primary performance bottlenecks</a> on the web. In the worst case, new navigation requires a DNS lookup, TCP handshake, two roundtrips to negotiate the TLS tunnel, and finally a minimum of another roundtrip for the actual HTTP request and response &mdash; that's five network roundtrips to get the first few bytes of the HTML document!</p> <p>Modern browsers <a href="http://www.igvita.com/posa/high-performance-networking-in-google-chrome/">try very hard</a> to anticipate and predict user activity to hide some of this latency, but speculative optimization is not a panacea: sometimes the browser doesn't have enough information, at other times it might guess wrong. This is why <strong>optimizing Time To First Byte (TTFB), and TLS TTFB in particular due to the extra roundtrips, is critical for delivering a consistent and optimized web experience.</strong></p> <h2>The why and the how of TTFB</h2> <p>According to the HTTP Archive, the size of the HTML document at 75th percentile is <a href="http://bigqueri.es/t/what-is-the-distribution-of-html-document-sizes-mobile-vs-desktop/156">~20KB+</a>, which means that a new TCP connection will incur multiple roundtrips (<a href="http://chimera.labs.oreilly.com/books/1230000000545/ch02.html#SLOW_START">due to slow-start</a>) to download this file - with IW4, a 20KB file will take 3 extra roundtrips, and <a href="http://chimera.labs.oreilly.com/books/1230000000545/ch02.html#_tuning_server_configuration">upgrading to IW10</a> will reduce that to 2 extra roundtrips.</p> <p>To minimize the impact of the extra roundtrips all <strong>modern browsers tokenize and parse received HTML incrementally</strong> and without waiting for the full file to arrive. Stream processing enables the browser to discover other critical resources, such as references to CSS stylesheets, JavaScript, and other assets as quickly as possible and initiate those requests while waiting for the remainder of the document. As a result, optimizing your TTFB and the content of those first bytes can make a big difference to performance of your application:</p> <ul> <li><strong>Don't buffer the entire response on the server.</strong> If you have partial content (e.g. page header), then flush it as early as possible to get the browser working on your behalf.</li> <li><strong>Optimize the contents of the first bytes</strong> by including references to other critical assets as early as possible.</li> </ul> <h2>Measuring "out of the box" NGINX TLS TTFB</h2> <p>With the theory of TTFB out of the way, let's now turn to the practical matter of picking and tuning the server to deliver the best results. One would hope that the default âout of the boxâ experience for most servers would do a good jobâ¦ unfortunately, that is not the case. Let's take a closer look nginx:</p> <ul> <li>Fresh Ubuntu server in ec2-west (micro instance) with nginx v1.4.4 (stable).</li> <li>The server is configured to serve a single 20KB (compressed) file.</li> <li>The TLS certificate is ~5KB and is using a 2048-bit key.</li> <li>The measurements are done with WebPageTest: 3G profile (300ms delay), Chrome (stable channel), Dulles location (~80ms actual RTT to the EC2 instance on the west coast).</li> </ul> <p>The total client to server roundtrip time is ~380ms. As a result, we would expect a regular HTTP connection to yield a TTFB of ~1140ms: 380ms for DNS, 380ms for TCP handshake, and 380ms for the HTTP request and (instant) response. For HTTPS, we would add another two RTTs to negotiate all the required parameters: 1140ms + 760ms, or ~1900ms (5 RTTs) in total. Well, that's the theory, let's now try the theory in practice!</p> <p><img src='http://www.igvita.com/posts/13/nginx-oob.png' class='center' style='max-width:760px;width:100%;' /></p> <p><strong>The HTTP TTFB is right on the mark (<a href="http://www.webpagetest.org/result/131210_AD_d59559928b917a897a946c865403e80f/1/details/">~1100ms</a>), but what in the world is going on with HTTPS?</strong> The TTFB reported by WebPageTest shows <a href="http://www.webpagetest.org/result/131210_PP_0889f2fc748958b9f1a931b3eab1f34c/1/details/">~2900ms</a>, which is an entire extra second over our expected value! Is it the cost of the RSA handshake and symmetric crypto? Nope. Running openssl benchmarks <a href="https://gist.github.com/igrigorik/7976678">on the server</a> shows that it takes ~2.5ms for a 2048-bit handshake, and we can stream ~100MB/s through aes-256. It's time to dig deeper.</p> <h2>Fixing the âlargeâ certificate bug in nginx</h2> <p>Looking at the <a href="http://cloudshark.org/captures/48955af54fe8?filter=tcp.stream%3D%3D2">tcpdump of our HTTPS session</a> we see the <code>ClientHello</code> record followed by <code>ServerHello</code> response ~380ms later. So far so good, but then something peculiar happens: <strong>the server sends ~4KB of its certificate and pauses to wait for an ACK from the client - huh?</strong> The server is using a recent Linux kernel (3.11) and is configured by default with IW10, which allows it to send up to 10KB, what's going on?</p> <p><img src='http://www.igvita.com/posts/13/cert-latency.png' class='center' style='max-width:760px;width:100%;' /></p> <p>After digging through the nginx source code, one stumbles <a href="https://github.com/nginx/nginx/commit/e52bddaaa90e64b2291f6e58ef1a2cff71604f6a#diff-0584d16332cf0d6dd9adb990a3c76a0cR539">onto this gem</a>. Turns out, any nginx version prior to 1.5.6 has this issue: <strong>certificates over 4KB in size incur an extra roundtrip, turning a two roundtrip handshake into a three roundtrip affair - yikes.</strong> Worse, in this particular case we trigger another unfortunate edge case in Windows TCP stack: the client ACKs the first few packets from the server, but then waits ~200ms before it triggers a delayed ACK for the last segment. In total, that results in extra 580ms of latency that we did not expect.</p> <p>Ok, let's try the current mainline nginx release (1.5.7) and see if we fare any better...</p> <p><img src='http://www.igvita.com/posts/13/nginx-mainline.png' class='center' style='max-width:760px;width:100%;' /></p> <p>Much better! After a simple upgrade the TLS TTFB is down to <a href="http://www.webpagetest.org/result/131210_AR_46f1678346fb22b7463cff5d304147a3/1/details/">~2300ms</a>, which is about 600ms lower than our first attempt: <strong>we've just eliminated the extra RTT incurred by nginx and the ~200ms delayed ACK on the client.</strong> That said, we are not out of the woods yet &mdash; there is still an extra RTT in there.</p> <h2>Optimizing the TLS record size</h2> <p><a href="http://www.igvita.com/2013/10/24/optimizing-tls-record-size-and-buffering-latency/">TLS record size can have a significant impact</a> on the page load time performance of your application. In this case, we run into this issue head first: nginx pumps data to the TLS layer, which in turn creates a 16KB record and then passes it to the TCP stack. So far so good, except that <strong>the server congestion window is less than 16KB for our new connection and we overflow the window, incurring an extra roundtrip while the data is buffered on the client.</strong> Fixing this requires making a quick patch to the nginx source:</p> <div class="highlight"><pre><code class="bash">diff nginx-1.5.7/src/event/ngx_event_openssl.c nginx-1.5.7-mtu/src/event/ngx_event_openssl.c 570c570 &lt; <span class="o">(</span>void<span class="o">)</span> BIO_set_write_buffer_size<span class="o">(</span>wbio, NGX_SSL_BUFSIZE<span class="o">)</span>; --- &gt; <span class="o">(</span>void<span class="o">)</span> BIO_set_write_buffer_size<span class="o">(</span>wbio, 16384<span class="o">)</span>; diff nginx-1.5.7/src/event/ngx_event_openssl.h nginx-1.5.7-mtu/src/event/ngx_event_openssl.h 107c107 &lt; <span class="c">#define NGX_SSL_BUFSIZE 16384</span> --- &gt; <span class="c">#define NGX_SSL_BUFSIZE 1400</span> </code></pre></div> <p><img src='http://www.igvita.com/posts/13/nginx-mtu.png' class='center' style='max-width:760px;width:100%;' /></p> <p><strong>After applying our two-line change and rebuilding the server our TTFB is down to <a href="http://www.webpagetest.org/result/131210_6N_9526d04a0ce33097806a2e5e6ba9f0fa/7/details/">~1900ms</a> &mdash; that's the 5 RTTs we expected at the start.</strong> In fact, it's easy to spot the difference from our previous run: the waterfall now shows the second RTT as content download time (blue section), whereas previously the browser couldn't process the HTML document until the very end. Success! But wait, what if I told you that we could do even better?</p> <h2>Enabling TLS False Start</h2> <p><strong><a href="http://tools.ietf.org/html/draft-bmoeller-tls-falsestart-00">TLS False Start</a> allows us to eliminate an extra roundtrip of latency within the <a href="http://chimera.labs.oreilly.com/books/1230000000545/ch04.html#TLS_HANDSHAKE">TLS handshake</a>:</strong> the client can send its encrypted application data (i.e. HTTP request) immediately after it has sent its <code>ChangeCipherSpec</code> and <code>Finished</code> records, without waiting for the server to confirm its settings. So, how do we enable TLS False Start?</p> <ul> <li>Chrome will use TLS False Start if it <a href="http://src.chromium.org/viewvc/chrome/trunk/src/net/third_party/nss/ssl/sslsecur.c?revision=235907#l401">detects</a> support for NPN negotiation and forward secrecy &mdash; NPN is an independent feature, but the presence of NPN support is used to <a href="https://www.imperialviolet.org/2012/04/11/falsestart.html">guard against broken implementations</a>.</li> <li>Firefox toggled TLS False Start support multiple times, but it will be <a href="https://bugzilla.mozilla.org/show_bug.cgi?id=942729">(re)enabled in M28</a>, and will also require an NPN advertisement and support for forward secrecy.</li> <li>IE10+ uses a <a href="http://blogs.msdn.com/b/ieinternals/archive/2012/08/01/internet-explorer-10-network-performance-improvements-first-available-pre-resolve-pre-connect-caching.aspx">combination of blacklist and a timeout</a> and doesn't require any additional TLS features.</li> <li>Apple <a href="http://opensource.apple.com/source/Security/Security-55471/libsecurity_ssl/lib/SecureTransport.h">landed TLS False Start</a> support in OSX 10.9, which hopefully means that its coming to Safari.</li> </ul> <p>In short, we need to enable NPN on the server, which in practice means that we need to rebuild nginx against OpenSSL 1.0.1a or higher &mdash; nothing more, nothing less. Let's do just that and see what happens...</p> <p><img src='http://www.igvita.com/posts/13/tttfb-compare.png' class='center' style='max-width:760px;width:100%;' /></p> <p>We started with a ~1800ms overhead for our TLS connection (nearly 5 extra RTTs); eliminated the extra certificate roundtrip after a nginx upgrade; cut another RTT by forcing a smaller record size; dropped an extra RTT from the TLS handshake thanks to TLS False Start. With all said and done, <strong>our TTTFB is down to ~1560ms, which is exactly one roundtrip higher than a regular HTTP connection.</strong> Now we're talking!</p> <p>Yes, TLS does add latency and processing overhead. That said, TLS is an unoptimized frontier and we can mitigate many of its costs - it's worth it. Our quick exploration with nginx is a case in point, and most other TLS servers have all the same issues we've outlined above. Let's get this fixed. <strong>TLS is not slow, it's unoptimized.</strong></p> <div class="feedflare"> <a href="http://feeds.igvita.com/~ff/igvita?a=5S-L5Oc1hBg:1n5MRLuD4Mw:yIl2AUoC8zA"><img src="http://feeds.feedburner.com/~ff/igvita?d=yIl2AUoC8zA" border="0"></img></a> </div><img src="http://feeds.feedburner.com/~r/igvita/~4/5S-L5Oc1hBg" height="1" width="1"/>
</content>
<feedburner:origLink>
http://www.igvita.com/2013/12/16/optimizing-nginx-tls-time-to-first-byte/
</feedburner:origLink>
</entry>
<entry>
<title type="html">Configuring & Optimizing WebSocket Compression</title>
<link rel="alternate" type="text/html" href="http://feeds.igvita.com/~r/igvita/~3/93m-zqRKf3s/"/>
<updated>2013-11-27T00:00:00-08:00</updated>
<id>
http://www.igvita.com/2013/11/27/configuring-and-optimizing-websocket-compression/
</id>
<content type="html">
<p><svg width="100" height="100" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 305.5 229.5" style="float:left; margin-right:1em;"> <path fill="#DF6200" d="M229.5,172.5h37.9V81.5l-42.7-42.7l-26.8,26.8l31.6,31.6V172.5z M267.5,191.5 h-55.2h-77.0l-31.6-31.6l13.4-13.4l26.1,26.1h53.7l-52.9-53.0l13.5-13.5l52.9,52.9v-53.7 l-26.0-26.0l13.3-13.3L132.0,0L67.2,0.0V0L0,0.0l37.8,37.8v0.1h0.2l78.2-0.0l27.7,27.7 l-40.5,40.5l-27.7-27.7V56.9H37.8v37.2l65.6,65.6l-26.7,26.7l42.7,42.7h64.8h121.1v-0.0 L267.5,191.5z"/> </svg></p> <p>Good news, browser support for the latest draft of â<a href="http://tools.ietf.org/html/draft-ietf-hybi-permessage-compression">Compression Extensions</a>â for WebSocket protocol &mdash; a much needed and overdue feature &mdash; will be landing in early 2014: Chrome M32+ (available in Canary already), and <a href="https://bugzilla.mozilla.org/show_bug.cgi?id=792831">Firefox</a> and <a href="https://bugs.webkit.org/show_bug.cgi?id=98840">Webkit</a> implementations should follow.</p> <p>Specifically, it <strong>enables the client and server to negotiate a compression algorithm and its parameters, and then selectively apply it to the data payloads of each WebSocket message</strong>: the server can compress delivered data to the client, and the client can compress data sent to the server.</p> <hr /> <ul class="post-toc" id="toc"> <li><a href="#negotiation">Negotiating compression support and parameters</a> <li><a href="#selective">Selective message compression</a> <li><a href="#scaling">Optimizing and scaling Deflate compression</a> <ul style="margin:0em"> <li><a href="#lz77">Optimizing LZ77 window size</a> <li><a href="#memLevel">Optimizing Deflate memLevel</a> <li><a href="#takeover">Context takeover</a> <li><a href="#parameters">Optimizing compression parameters</a> </ul> <li><a href="#deploying">Deploying WebSocket compression</a> </ul> <h2 id="negotiation">Negotiating compression support and parameters <a href="#negotiation">#</a> <a class="toc-arrow" href="#toc">&uarr;</a></h2> <p>Per-message compression is a WebSocket protocol extension, which means that it must be negotiated as part of the WebSocket handshake. Further, unlike a regular HTTP request (e.g. XMLHttpRequest initiated by the browser), WebSocket also allows us to negotiate compression parameters in both directions (client-to-server and server-to-client). That said, let's start with the simplest possible case:</p> <div class="highlight"><pre><code class="http"><span class="nf">GET</span> <span class="nn">/socket</span> <span class="kr">HTTP</span><span class="o">/</span><span class="m">1.1</span> <span class="na">Host</span><span class="o">:</span> <span class="l">thirdparty.com</span> <span class="na">Origin</span><span class="o">:</span> <span class="l">http://example.com</span> <span class="na">Connection</span><span class="o">:</span> <span class="l">Upgrade</span> <span class="na">Upgrade</span><span class="o">:</span> <span class="l">websocket</span> <span class="na">Sec-WebSocket-Version</span><span class="o">:</span> <span class="l">13</span> <span class="na">Sec-WebSocket-Key</span><span class="o">:</span> <span class="l">dGhlIHNhbXBsZSBub25jZQ==</span> <span class="na">Sec-WebSocket-Extensions</span><span class="o">:</span> <span class="l">permessage-deflate</span> </code></pre></div> <div class="highlight"><pre><code class="http"><span class="kr">HTTP</span><span class="o">/</span><span class="m">1.1</span> <span class="m">101</span> <span class="ne">Switching Protocols</span> <span class="na">Upgrade</span><span class="o">:</span> <span class="l">websocket</span> <span class="na">Connection</span><span class="o">:</span> <span class="l">Upgrade</span> <span class="na">Access-Control-Allow-Origin</span><span class="o">:</span> <span class="l">http://example.com</span> <span class="na">Sec-WebSocket-Accept</span><span class="o">:</span> <span class="l">s3pPLMBiTxaQ9kYGzzhZRbK+xOo=</span> <span class="na">Sec-WebSocket-Extensions</span><span class="o">:</span> <span class="l">permessage-deflate</span> </code></pre></div> <p>The client initiates the negotiation by advertising the <code>permessage-deflate</code> extension in the <code>Sec-Websocket-Extensions</code> header. In turn, the server must confirm the advertised extension by echoing it in its response.</p> <p>If the server omits the extension confirmation then the use of permessage-deflate is declined, and both the client and server proceed without it - i.e. the handshake completes and messages won't be compressed. Conversely, if the extension negotiation is successful, both the client and server can compress transmitted data as necessary:</p> <ul> <li><strong>Current standard uses Deflate compression.</strong></li> <li><strong>Compression is only applied to application data:</strong> control frames and frame headers are unaffected.</li> <li><strong>Both client and server can selectively compress individual frames:</strong> if the frame is compressed, the <code>RSV1</code> bit in the WebSocket frame header is set.</li> </ul> <h2 id="selective">Selective message compression <a href="#selective">#</a> <a class="toc-arrow" href="#toc">&uarr;</a></h2> <p>Selective compression is a particularly interesting and a useful feature. Just because we've negotiated compression support, doesn't mean that all messages must be compressed! After all, if the payload is already compressed (e.g. image data or any other compressed payload), then running deflate on each frame would unnecessarily waste CPU cycles on both ends. To avoid this, WebSocket allows both the server and client to selectively compress individual messages.</p> <p><img src='http://www.igvita.com/posts/13/ws-compression.png' class='center' style='max-width:547px;width:100%;' /></p> <p>How do the server and client know when to compress data? This is where your choice of a WebSocket server and API can make a big difference: a naive implementation will simply compress all message payloads, whereas a <strong>smart server may offer an additional API to indicate which payloads should be compressed.</strong></p> <p>Similarly, the browser can selectively compress transmitted payloads to the server. However, this is where we run into our first limitation: the WebSocket browser API does not provide any mechanism to signal whether the payload should be compressed. As a result, <strong>the current implementation in Chrome compresses all payloads</strong> - if you're already transferring compressed data over WebSocket without deflate extension then this is definitely something you should consider as it may add unnecessary overhead on both sides of the connection.</p> <div class="callout"> In theory, in absence of an official API, or a per-message flag to indicate a compressed message, the UA could run a âdata randomnessâ test to see if the data should be compressed. However, this by itself can add non-trivial processing overhead. </div> <h2 id="scaling">Optimizing and scaling Deflate compression <a href="#scaling">#</a> <a class="toc-arrow" href="#toc">&uarr;</a></h2> <p>Compressed payloads can significantly reduce the amount of transmitted data, which leads to bandwidth savings and faster message delivery. That said, there are some costs too! Deflate uses a combination of <a href="http://en.wikipedia.org/wiki/LZ77_and_LZ78">LZ77</a> and <a href="http://en.wikipedia.org/wiki/Huffman_coding">Huffman coding</a> to compress data: first, LZ77 is used to eliminate duplicate strings; second, Huffman coding is used to encode common bit sequences with shorter representations.</p> <p>By default, <strong>enabling compression will add at least ~300KB of extra memory overhead per WebSocket connection</strong> - arguably, not much, but if your server is juggling a large number of WebSocket connections, or if the client is running on a memory-limited device, then this is something that should be taken into account. The exact calculation based on zlib implementation of Deflate is <a href="http://www.zlib.net/zlib_tech.html">as follows</a>:</p> <pre> compressor = (1 << (windowBits + 2)) + (1 << (memLevel + 9)) decompressor = (1 << windowBits) + 1440 * 2 * sizeof(int) total = compressor + decompressor </pre> <p>Both peers maintain separate compression and decompression contexts, each of which require a separate LZ77 window buffer (as defined by <code>windowBits</code>), plus additional overhead for the Huffman tree and other compressor and decompressor overhead. The default settings are:</p> <ul> <li>compressor: windowBits = 15, memLevel = 8 â ~256KB</li> <li>decompressor: windowBits = 15 â ~44KB</li> </ul> <p>The good news is that <strong>permessage-deflate allows us to customize the size of the LZ77 window and thus limit the memory overhead via two extension parameters:</strong> <code>{client, server}_no_context_takeover</code> and <code>{client, server}_max_window_bits</code>. Let's take a look under the hood...</p> <h3 id="lz77">Optimizing LZ77 window size <a href="#lz77">#</a> <a class="toc-arrow" href="#toc">&uarr;</a></h3> <p>A full discussion of LZ77 and Huffman coding is outside the scope of this post, but to understand the above extension parameters, let's first take a small detour to understand what we are configuring and the inherent tradeoffs between memory and compression performance.</p> <div class='ytvideo' id='SWBkneyTyPU'></div> <p>The <code>windowBits</code> parameter is customizing the size of the âsliding windowâ used by the LZ77 algorithm. Above video is a great visual demonstration of LZ77 at work: the algorithm maintains a âsliding windowâ of previously seen data and replaces repeated strings (indicated in red) with back-references (e.g. go back X characters, copy Y characters) - that's LZ77 in a nutshell. As a result, <strong>the larger the window, the higher the likelihood that LZ77 will find and eliminate duplicate strings</strong>.</p> <p>How large is the LZ77 sliding window? By default, the window is initialized to 15 bits, which translates to 2<sup>15</sup> bits (32KB) of space. However, we can customize the size of the sliding window as part of the WebSocket handshake:</p> <div class="highlight"><pre><code class="http"><span class="nf">GET</span> <span class="nn">/socket</span> <span class="kr">HTTP</span><span class="o">/</span><span class="m">1.1</span> <span class="na">Upgrade</span><span class="o">:</span> <span class="l">websocket</span> <span class="na">Sec-WebSocket-Key</span><span class="o">:</span> <span class="l">...</span> <span class="na">Sec-WebSocket-Extensions</span><span class="o">:</span> <span class="l">permessage-deflate;</span> <span class="l">client_max_window_bits; server_max_window_bits=10</span> </code></pre></div> <ul> <li>The client advertises that it supports custom window size via <code>client_max_window_bits</code></li> <li>The client requests that the server should use a window of size 2<sup>10</sup> (1KB)</li> </ul> <div class="highlight"><pre><code class="http"><span class="kr">HTTP</span><span class="o">/</span><span class="m">1.1</span> <span class="m">101</span> <span class="ne">Switching Protocols</span> <span class="na">Connection</span><span class="o">:</span> <span class="l">Upgrade</span> <span class="na">Sec-WebSocket-Accept</span><span class="o">:</span> <span class="l">...</span> <span class="na">Sec-WebSocket-Extensions</span><span class="o">:</span> <span class="l">permessage-deflate;</span> <span class="l">server_max_window_bits=10</span> </code></pre></div> <ul> <li>The server confirms that it will use a 2<sup>10</sup> (1KB) window size</li> <li>The server opts-out from requesting a custom client window size</li> </ul> <p><strong>Both the client and server must maintain the same sliding windows to exchange data: one buffer for client â server compression context, and one buffer for server â client context.</strong> As a result, by default, we will need two 32KB buffers (default window size is 15 bits), plus other compressor overhead. However, we can negotiate a smaller window size: in the example above, we limit the server â client window size to 1KB.</p> <p>Why not start with the smaller buffer? Simple, the smaller the window the less likely it is that LZ77 will find an appropriate back-reference. That said, the performance will vary based on the type and amount of transmitted data and there is no single rule of thumb for best window size. To get the best performance, test different window sizes on your data! Then, where needed, decrease window size to reduce memory overhead.</p> <h3 id="memLevel">Optimizing Deflate memLevel <a href="#memLevel">#</a> <a class="toc-arrow" href="#toc">&uarr;</a></h3> <p>The memLevel parameter controls the amount of memory allocated for internal compression state: when set to 1, it uses the least memory, but slows down the compression algorithm and reduces the compression ratio; when set to 9, it uses the most memory and delivers the best performance. The default memLevel is set to 8, which results in ~133KB of required memory overhead for the compressor.</p> <p><strong>Note that the decompressor does not need to know the memLevel chosen by the compressor.</strong> As a result, the peers do not need to negotiate this setting in the handshake - they are both free to customize this value as they wish. The server can tune this value as required to tradeoff speed, compression ratio, and memory - once again, the best setting will vary based on your data stream and operational requirements.</p> <div class="callout"> Unfortunately, the client, which in this case is the browser user-agent does not provide any API to customize the memLevel of the compressor: memLevel = 8 is used as a default value in all cases. Similar to the missing per-message compression flag, perhaps this is a feature that can be added to a future revision of the WebSocket spec. </div> <h3 id="takeover">Context takeover <a href="#takeover">#</a> <a class="toc-arrow" href="#toc">&uarr;</a></h3> <p><strong>By default, the compression and decompression contexts are persisted across different WebSocket messages - i.e. the sliding window from previous message is used to encode content of the next message.</strong> If messages are similar &mdash; as they usually are &mdash; this improves the compression ratio. However, the downside is that the context overhead is a fixed cost for the entire lifetime of the connection - i.e. memory must be allocated at the beginning and must be maintained until the connection is closed.</p> <p>Well, what if we relaxed this constraint and instead allowed the peers to reset the context between the different messages? That's what âno context takeoverâ option is all about:</p> <div class="highlight"><pre><code class="http"><span class="nf">GET</span> <span class="nn">/socket</span> <span class="kr">HTTP</span><span class="o">/</span><span class="m">1.1</span> <span class="na">Upgrade</span><span class="o">:</span> <span class="l">websocket</span> <span class="na">Sec-WebSocket-Key</span><span class="o">:</span> <span class="l">...</span> <span class="na">Sec-WebSocket-Extensions</span><span class="o">:</span> <span class="l">permessage-deflate;</span> <span class="l">client_max_window_bits; server_max_window_bits=10;</span> <span class="l">client_no_context_takeover; server_no_context_takeover</span> </code></pre></div> <ul> <li>Client advertises that it will disable context takeover</li> <li>Client requests that the server also disables context takeover</li> </ul> <div class="highlight"><pre><code class="http"><span class="kr">HTTP</span><span class="o">/</span><span class="m">1.1</span> <span class="m">101</span> <span class="ne">Switching Protocols</span> <span class="na">Connection</span><span class="o">:</span> <span class="l">Upgrade</span> <span class="na">Sec-WebSocket-Accept</span><span class="o">:</span> <span class="l">...</span> <span class="na">Sec-WebSocket-Extensions</span><span class="o">:</span> <span class="l">permessage-deflate;</span> <span class="l">server_max_window_bits=10; client_max_window_bits=12</span> <span class="l">client_no_context_takeover; server_no_context_takeover</span> </code></pre></div> <ul> <li>Server acknowledges the client âno context takeoverâ recommendation</li> <li>Server indicates that it will disable context takeover</li> </ul> <p><strong>Disabling âcontext takeoverâ prevents the peer from using the compressor context from a previous message to encode contents of the next message.</strong> In other words, each message is encoded with its own sliding window and Huffman tree.</p> <p>The upside of disabling context takeover is that both the client and server can reset their contexts between different messages, which significantly reduces the total overhead of the connection when its idle. That said, the downside is that compression performance will likely suffer as well. How much? You guessed it, the answer depends on the actual application data being exchanged.</p> <div class="callout"> Note that even without âno_context_takeoverâ negotiation, the decompressor should be able to decode both types of messages. That said, the explicit negotiation is what allows us to know that it is safe to reset the context on the receiver. </div> <h3 id="parameters">Optimizing compression parameters <a href="#parameters">#</a> <a class="toc-arrow" href="#toc">&uarr;</a></h3> <p>Now that we know what we're tweaking, a <a href="https://gist.github.com/igrigorik/7680914">simple ruby script</a> can help us iterate over all of the options (memLevel and window size) to pick the optimal settings. For the sake of an example, let's compress the GitHub timeline:</p> <div class="highlight"><pre><code class="bash"><span class="nv">$&gt;</span> curl https://github.com/timeline.json -o timeline.json <span class="nv">$&gt;</span> ruby compare.rb timeline.json Original file <span class="o">(</span>timeline.json<span class="o">)</span> size: 30437 bytes Window size: 8 bits <span class="o">(</span>256 bytes<span class="o">)</span> memLevel: 1, compressed size: 19472 bytes <span class="o">(</span>36.03% reduction<span class="o">)</span> memLevel: 9, compressed size: 15116 bytes <span class="o">(</span>50.34% reduction<span class="o">)</span> Window size: 11 bits <span class="o">(</span>2048 bytes<span class="o">)</span> memLevel: 1, compressed size: 9797 bytes <span class="o">(</span>67.81% reduction<span class="o">)</span> memLevel: 9, compressed size: 8062 bytes <span class="o">(</span>73.51% reduction<span class="o">)</span> Window size: 15 bits <span class="o">(</span>32768 bytes<span class="o">)</span> memLevel: 1, compressed size: 8327 bytes <span class="o">(</span>72.64% reduction<span class="o">)</span> memLevel: 9, compressed size: 7027 bytes <span class="o">(</span>76.91% reduction<span class="o">)</span> </code></pre></div> <p>The smallest allowed window size (256 bytes) provides ~50% compression, and raising the window to 2KB, takes us to ~73%! From there, it is diminishing returns: 32KB window yields only a few extra percent (see <a href="https://gist.github.com/igrigorik/7680914#file-github-timeline-sh">full output</a>). Hmm! If I was streaming this data over a WebSocket, a 2KB window size seems like a reasonable optimization.</p> <h2 id="deploying">Deploying WebSocket compression <a href="#deploying">#</a> <a class="toc-arrow" href="#toc">&uarr;</a></h2> <p>Customizing LZ77 window size and context takeover are advanced optimizations. Most applications will likely get the best performance by simply using the defaults (32KB window size and shared sliding window). That said, it is useful to understand the incurred overhead (upwards of 300KB per connection), and the knobs that can help you tweak these parameters!</p> <p>Looking for a WebSocket server that supports per-message compression? Rumor has it, Jetty, Autobahn and WebSocket++ already support it, and other servers (and clients) are sure to follow. For a deep-dive on the negotiation workflow, frame layouts, and more, check out the <a href="http://tools.ietf.org/html/draft-ietf-hybi-permessage-compression">official specification</a>.</p> <p><em>P.S. For more WebSocket optimization tips: <a href="http://chimera.labs.oreilly.com/books/1230000000545/ch17.html">WebSocket chapter in High Performance Browser Networking</a>.</em></p> <div class="feedflare"> <a href="http://feeds.igvita.com/~ff/igvita?a=93m-zqRKf3s:59j2ezUXI-s:yIl2AUoC8zA"><img src="http://feeds.feedburner.com/~ff/igvita?d=yIl2AUoC8zA" border="0"></img></a> </div><img src="http://feeds.feedburner.com/~r/igvita/~4/93m-zqRKf3s" height="1" width="1"/>
</content>
<feedburner:origLink>
http://www.igvita.com/2013/11/27/configuring-and-optimizing-websocket-compression/
</feedburner:origLink>
</entry>
<entry>
<title type="html">Optimizing TLS Record Size & Buffering Latency</title>
<link rel="alternate" type="text/html" href="http://feeds.igvita.com/~r/igvita/~3/JzML3cOc7Y0/"/>
<updated>2013-10-24T00:00:00-07:00</updated>
<id>
http://www.igvita.com/2013/10/24/optimizing-tls-record-size-and-buffering-latency/
</id>
<content type="html">
<p><img src='http://www.igvita.com/posts/13/tls-layer.png' class='left' /><strong>TLS record size can have significant impact on the page load time performance of your application.</strong> In fact, in the worst case, which unfortunately is also the most common case on the web today, it can delay processing of received data by up to several roundtrips! On mobile networks, this can translate to hundreds of milliseconds of unnecessary latency.</p> <p>The good news is, this is a relatively simple thing to fix if your TLS server supports it. However, first let's take a step back and understand the problem: <strong>what is a TLS record, why do we need it, and why does record size affect latency of our applications?</strong></p> <h2>Encryption, authentication, and data integrity</h2> <p>TLS provides three essential services: <a href="http://chimera.labs.oreilly.com/books/1230000000545/ch04.html">encryption, authentication, and data integrity</a>. Encryption obfuscates the data, authentication provides a mechanism to verify the identity of the peer, and integrity provides a mechanism to detect message tampering and forgery.</p> <p>Each TLS connection begins with a handshake where the peers negotiate the ciphersuite, establish the secret keys for the connection, and authenticate their identities (on the web, typically we only authenticate the identity of the server - e.g. is this really my bank?). Once these steps are complete, we can begin transferring application data between the client and server. This is where data integrity and record size optimization enter into the picture.</p> <p><img src='http://www.igvita.com/posts/13/tls-record-1.png' class='center' style='max-width:510px;width:100%;' /></p> <p>Before the application data is encrypted, it is divided into smaller chunks of up to 16KB in size and each of the resulting pieces is signed with a message authentication code (MAC). The resulting data chunk is then encrypted, and the MAC, plus some other protocol metadata forms a âTLS recordâ, which is then forwarded to the peer:</p> <p><img src='http://www.igvita.com/posts/13/tls-record.png' class='center' style='max-width:760px;width:100%;' /></p> <p>The receiver reverses the sequence. First, the bytes are aggregated until one or more complete records are in the buffer (as above diagram shows, each record specifies its length in the header), and once the entire record is available, the payload is then decrypted, the MAC is computed once more for the decrypted data, and is finally verified against the MAC contained in the record. If the two hashes match, data integrity is assured, and TLS finally releases the data to the application above it.</p> <h2>Head-of-line blocking, TLS records, and latency</h2> <p>TLS runs over TCP, and TCP promises in order delivery of all transferred packets. As a result, <a href="http://chimera.labs.oreilly.com/books/1230000000545/ch02.html#TCP_HOL">TCP suffers from head-of-line (HOL) blocking</a> where a lost packet may hold all other received packets in the buffer until it is successfully retransmitted - otherwise, the packets would be delivered out of order! This is a well-known tradeoff for any protocol that runs on top of a reliable and in-order transport like TCP.</p> <p>However, in case of TLS, we have an extra layer of buffering due to the integrity checks! Once TCP delivers the packets to the TLS layer above it, we must first accumulate the entire record, then verify its MAC checksum, and only when that passes, can we release the data to the application above it. As a result, if the server emits data in 16KB record chunks, then the receiver must also read data 16KB at a time.</p> <p><img src='http://www.igvita.com/posts/13/tls-record-2.png' class='center' style='max-width:510px;width:100%;' /></p> <p>In other words, <strong>even if the receiver has 15KB of the record in the buffer and is waiting for the last packet to complete the 16KB record, the application can't read it until the entire record is received and the MAC is verified</strong> - therein lies our latency problem! If any packet is lost, we will incur a minimum of an additional RTT to retransmit it. Similarly, if the record happens to exceed the current TCP congestion window, then it will take a minimum of two RTTs before the application can process sent data.</p> <h2>Eliminating TLS latency</h2> <p>The larger the TLS record size, the higher the likelihood that we may incur an additional roundtrip due to a TCP retransmission or "overflow" of the congestion window. That said, the fix is also relatively simple: <strong>send smaller records.</strong> In fact, to eliminate this problem entirely, configure your TLS record size to <a href="http://chimera.labs.oreilly.com/books/1230000000545/ch04.html#TLS_RECORD_SIZE">fit into a single TCP segment</a>.</p> <p><img src='http://www.igvita.com/posts/13/tls-record-3.png' class='center' style='max-width:511px;width:100%;' /></p> <p>If the TCP congestion window is small (i.e. <a href="http://chimera.labs.oreilly.com/books/1230000000545/ch02.html#SLOW_START">during slow-start</a>), or if we're sending interactive data that should be processed as soon as possible (in other words, most HTTP traffic), <strong>then small record size helps mitigate costly latency overhead of yet another layer of buffering</strong>.</p> <h2>Configuring your TLS server</h2> <p>The bad news is that many TLS servers do not provide an easy way to configure TLS record size and instead use the default maximum of 16 KB. The good news is, if you are using HAProxy to terminate TLS, then you are in luck, and otherwise, you may need to fiddle with the source of your server (assuming it is open source):</p> <ul> <li>HAProxy exposes <a href="http://cbonte.github.io/haproxy-dconv/configuration-1.5.html#3.2-tune.ssl.maxrecord">tune.ssl.maxrecord</a> as a config option.</li> <li>Nginx hardcodes <a href="https://github.com/nginx/nginx/blob/master/src/event/ngx_event_openssl.h#L97">16KB size in ngx_event_openssl</a>, which you can change and recompile from source.</li> <li>For other servers, check the documentation!</li> </ul> <p>Web browsing is latency bound and eliminating extra roundtrips is critical for delivering better performance. All Google servers are configured to begin new connections with TLS records that fit into a single network segment - it is critical to get useful data to the client as quickly as possible. Your server should do so as well!</p> <p><em>P.S. For more TLS optimization tips, check out the <a href="http://chimera.labs.oreilly.com/books/1230000000545/ch04.html">TLS chapter in High Performance Browser Networking</a>.</em></p> <div class="feedflare"> <a href="http://feeds.igvita.com/~ff/igvita?a=JzML3cOc7Y0:_AVfWRauaoA:yIl2AUoC8zA"><img src="http://feeds.feedburner.com/~ff/igvita?d=yIl2AUoC8zA" border="0"></img></a> </div><img src="http://feeds.feedburner.com/~r/igvita/~4/JzML3cOc7Y0" height="1" width="1"/>
</content>
<feedburner:origLink>
http://www.igvita.com/2013/10/24/optimizing-tls-record-size-and-buffering-latency/
</feedburner:origLink>
</entry>
<entry>
<title type="html">Retrospective: High Performance Browser Networking</title>
<link rel="alternate" type="text/html" href="http://feeds.igvita.com/~r/igvita/~3/-ehrQc7PJyQ/"/>
<updated>2013-09-29T00:00:00-07:00</updated>
<id>
http://www.igvita.com/2013/09/29/retrospective-high-performance-browser-networking/
</id>
<content type="html">
<p><img src='http://www.igvita.com/posts/13/hpbn-book.jpg' class='left' style='border:1px #333 solid' />This past Friday the first print copies of <a href="http://www.amazon.com/High-Performance-Browser-Networking-performance/dp/1449344763/">High Performance Browser Networking</a> landed on my desk. After carrying it in my head for the past year, it was an interesting feeling to finally hold something tangible: a sense of relief, excitement, and <em>"man, I hope they like it."</em></p> <p>With the book in hand, I could finally put a mental checkmark on it, which also prompted a retrospective into the entire process from start to finish. A self-professed <a href="http://en.wikipedia.org/wiki/Quantified_Self">quantified self</a> geek that I am, it should not surprise you to know that I've kept a detailed log along the way. Not knowing what I was getting myself into when I first started, it's interesting to now look back at the lessons learned and patterns that emerged. Let's take a look...</p> <h2>Writing HPBN by the numbers</h2> <table> <thead> <tr> <th>Chapter</th> <th>Writing (hours)</th> <th>Research + Review (hours)</th> <th>Words</th> </tr> </thead> <tfoot> <tr><td></td><td>412</td><td>506</td><td>90,056</td></tr> </tfoot> <tr><td><a href="http://chimera.labs.oreilly.com/books/1230000000545/pr02.html">Preface</a></td><td>2</td><td>1</td><td>900</td></tr> <tr><td><a href="http://chimera.labs.oreilly.com/books/1230000000545/ch01.html">Primer on Latency and Bandwidth</a></td><td>7</td><td>5</td><td>2,930</td></tr> <tr><td><a href="http://chimera.labs.oreilly.com/books/1230000000545/ch02.html">Building Blocks of TCP</a></td><td>25</td><td>15</td><td>5,533</td></tr> <tr><td><a href="http://chimera.labs.oreilly.com/books/1230000000545/ch03.html">Building Blocks of UDP</a></td><td>12</td><td>8</td><td>2,759</td></tr> <tr><td><a href="http://chimera.labs.oreilly.com/books/1230000000545/ch04.html">Transport Layer Security (TLS)</a></td><td>27</td><td>38</td><td>7,362</td></tr> <tr><td><a href="http://chimera.labs.oreilly.com/books/1230000000545/ch05.html">Introduction to Wireless Networks</a></td><td>10</td><td>16</td><td>2,763</td></tr> <tr><td><a href="http://chimera.labs.oreilly.com/books/1230000000545/ch06.html">WiFi</a></td><td>7</td><td>21</td><td>2,590</td></tr> <tr><td><a href="http://chimera.labs.oreilly.com/books/1230000000545/ch07.html">Mobile Networks</a></td><td>64</td><td>143</td><td>11,133</td></tr> <tr><td><a href="http://chimera.labs.oreilly.com/books/1230000000545/ch08.html">Optimizing for Mobile Networks</a></td><td>9</td><td>21</td><td>4,015</td></tr> <tr><td><a href="http://chimera.labs.oreilly.com/books/1230000000545/ch09.html">Brief History of HTTP</a></td><td>7</td><td>6</td><td>2,475</td></tr> <tr><td><a href="http://chimera.labs.oreilly.com/books/1230000000545/ch10.html">Primer on Web Performance</a></td><td>37</td><td>16</td><td>4,943</td></tr> <tr><td><a href="http://chimera.labs.oreilly.com/books/1230000000545/ch11.html">HTTP 1.X</a></td><td>28</td><td>15</td><td>5,156</td></tr> <tr><td><a href="http://chimera.labs.oreilly.com/books/1230000000545/ch12.html">HTTP 2.X</a></td><td>37</td><td>46</td><td>6,464</td></tr> <tr><td><a href="http://chimera.labs.oreilly.com/books/1230000000545/ch13.html">Optimizing Application Delivery</a></td><td>23</td><td>15</td><td>4,776</td></tr> <tr><td><a href="http://chimera.labs.oreilly.com/books/1230000000545/ch14.html">Primer on Browser Networking</a></td><td>5</td><td>3</td><td>1,618</td></tr> <tr><td><a href="http://chimera.labs.oreilly.com/books/1230000000545/ch15.html">XMLHttpRequest</a></td><td>16</td><td>14</td><td>5,109</td></tr> <tr><td><a href="http://chimera.labs.oreilly.com/books/1230000000545/ch16.html">Server-Sent Events (SSE)</a></td><td>6</td><td>11</td><td>1,681</td></tr> <tr><td><a href="http://chimera.labs.oreilly.com/books/1230000000545/ch17.html">WebSocket</a></td><td>38</td><td>25</td><td>5,583</td></tr> <tr><td><a href="http://chimera.labs.oreilly.com/books/1230000000545/ch18.html">WebRTC</a></td><td>52</td><td>87</td><td>12,266</td></tr> </table> <p><strong>In total, I've spent 412 hours staring into my text editor.</strong> This number is exact and based on RescueTime logging the foreground window. The research and review time was a harder one to track, since that involved online and offline time (e.g., reading books, research papers, etc.), but I'm confident of the data: writing time resolution is down to minutes, research and review are down to hours, with a total of 918 hours and 90,056 words.</p> <p>Alas, writing, research, and review are not the end of it. RescueTime tells me that I've spent <strong>46 hours iterating on feedback</strong> from early release iterations - discussions, followup, and so on. I've also logged <strong>29 hours in OmniGraffle</strong>, creating and adjusting all the diagrams. Finally, I've spent <strong>9 hours fiddling with the tooling</strong> - waiting for builds, configuration, debugging errors, etc. <strong>All said and done the grand total adds up to just north of 1,000 hours</strong>.</p> <h2>Taking it a day at a time</h2> <p>The whole process from getting the nod from the O'Reilly team, to handing off the content to the production crew took 381 days. In that time, my Git repository tells me that I've had 239 days with at least one commit, plus another 32 days where I didn't touch the repo but still spent time working on the book: <strong>271 active days.</strong></p> <p><img src='http://www.igvita.com/posts/13/hpbn-commits.png' class='center' style='max-width:602px;width:100%;' /></p> <p>I wouldn't call myself a morning person, but writing in the evenings after a full day at work didn't yield the results I wanted. Instead, as the Git commit log shows, I've tried to carve out a dedicated chunk of time each morning. Typically, that meant 7-9AM on weekdays, and 8-noon on weekends (hence the commit spikes between 12-2PM). The drop in number of commits on Tuesday/Thursday is interesting as that was not intentional - looking back it seems to coincide with higher amount of time spent on research and review.</p> <ul> <li><strong>1.72 hours (writing) / day</strong></li> <li><strong>1.59 pages / day</strong> or <strong>0.92 pages / hour</strong></li> <li><strong>377 words / day</strong> or <strong>219 words / hour</strong></li> </ul> <p>Reminding myself that a "<a href="http://www.humnet.ucla.edu/humnet/english/wwwroot2/ta/hyperteach/pdfs/shitty.pdf">shitty first draft</a>" should be the initial goal was a continuous struggle - 200 words/hour is not a fast clip. Then again, seeing another page or two appear at the end of the PDF was a great reward and all I was aiming for each day. My best day ever was 11 pages &mdash; a full weeks work in one day! Of course, then I had to rewrite most of it. Conversely, there were plenty of days where all I managed was a paragraph or two.</p> <h2>Lessons learned along the way</h2> <p>Collecting above data along the way proved to be a valuable feedback mechanism - e.g. focusing on working in the mornings, setting realistic expectations, and tracking progress. Case in point, my early delivery estimates were off by months and by the end I could project down to a week. Also, a few other interesting lessons learned:</p> <ul> <li><strong>Consistency is key</strong> - things get in the way (e.g. work, travel, etc.), but showing up is key.</li> <li><strong>Getting early feedback is invaluable</strong> - <a href="http://hpbn.co/">O'Reilly's Atlas</a> toolchain worked really well.</li> <li>Incremental milestones and deadlines are a necessary forcing function.</li> <li>Nothing helps clarify your thinking and expose the flaws and inconsistencies than attempting to put it down on paper in simple terms.</li> </ul> <p>Last point in particular is one of the reasons I keep this blog - writing helps me understand the topic in a way that reading and talking about it never does. Working on HPBN forced me to clarify a lot of important details I've overlooked before; relearn topics I thought I understood but really didn't; learn a lot of new material.</p> <p>With all that said, ultimately you'll have to be the judge on whether the book actually delivers - hope you learn a useful thing or two as well! Speaking of which, do check it out, and if you do, please leave a review!</p> <ul> <li><a href="http://www.amazon.com/High-Performance-Browser-Networking-performance/dp/1449344763/">HPBN on Amazon</a></li> <li><a href="http://shop.oreilly.com/product/0636920028048.do">HPBN on O'Reilly</a> (Kindle, PDF)</li> <li><a href="https://play.google.com/store/books/details/Ilya_Grigorik_High_Performance_Browser_Networking?id=tf--AAAAQBAJ&amp;hl=en">HPBN on Google Play Store</a></li> <li><a href="http://hpbn.co/">HPBN on Atlas</a> (Free online version)</li> </ul> <div class="feedflare"> <a href="http://feeds.igvita.com/~ff/igvita?a=-ehrQc7PJyQ:zIIIYDg6gLg:yIl2AUoC8zA"><img src="http://feeds.feedburner.com/~ff/igvita?d=yIl2AUoC8zA" border="0"></img></a> </div><img src="http://feeds.feedburner.com/~r/igvita/~4/-ehrQc7PJyQ" height="1" width="1"/>
</content>
<feedburner:origLink>
http://www.igvita.com/2013/09/29/retrospective-high-performance-browser-networking/
</feedburner:origLink>
</entry>
<entry>
<title type="html">Automating DPR switching with Client-Hints</title>
<link rel="alternate" type="text/html" href="http://feeds.igvita.com/~r/igvita/~3/019fSVpyQq4/"/>
<updated>2013-08-29T00:00:00-07:00</updated>
<id>
http://www.igvita.com/2013/08/29/automating-dpr-switching-with-client-hints/
</id>
<content type="html">
<p><em>Note: Updated Oct 30, 2013 to reflect <a href="https://github.com/igrigorik/http-client-hints/blob/master/draft-grigorik-http-client-hints-01.txt">new Client Hints draft</a>.</em></p> <p><img src='http://www.igvita.com/posts/13/dpr.png' class='left' />High-density displays pack multiple physical pixels per CSS pixel, and the ratio between the two is known as the <em>device pixel ratio</em> (DPR). To deliver the best visual quality, a HiDPI image that matches the DPR value of the device should be served to the client. The question is, how?</p> <p>The answer to date has been to use CSS media queries, but that's both cumbersome and insufficient: it's a CSS only solution, <code>img</code> tag is more or less useless, etc. In response, we now have a number of proposals on the table: <a href="http://www.w3.org/TR/html-picture-element/">picture element</a>, <a href="http://www.w3.org/html/wg/drafts/srcset/w3c-srcset/">srcset</a>, and the lesser known <a href="http://dev.w3.org/csswg/css-images/#image-set-notation">image-set</a>. Much ink has already been spilled on the pros and cons of each, but the new and notable development is that WebKit has recently <a href="https://www.webkit.org/blog/2910/improved-support-for-high-resolution-displays-with-the-srcset-image-attribute/">landed an implementation of srcset</a>.</p> <h2>srcset is good enough (tm)</h2> <p>Both srcset and picture tag are much more ambitious and go well beyond DPR-switching, but it's important to note that the current WebKit implementation <a href="https://www.webkit.org/blog/2910/improved-support-for-high-resolution-displays-with-the-srcset-image-attribute/">tackles a subset</a> of the larger spec - specifically, just the DPR switching. You can look at this in two ways: as a failure because we haven't come up with "the solution", or as a necessary and well overdue positive first step.</p> <p>I happen to be in the latter group. In fact, I think it's a big positive step (<a href="https://docs.google.com/document/d/1xqqbd5u8Gudjs7Ubp3sBOIssbmtenY9t_te48or2_Ok/edit">it's good enough</a>), and I hope other browsers <a href="https://groups.google.com/a/chromium.org/d/msg/blink-dev/dA8lbqLvaXA/b39vij5VPoMJ">will</a> <a href="https://bugzilla.mozilla.org/show_bug.cgi?id=870021">follow</a>. Having said that, I do have some reservations as well. Let's take a look at what's on the table:</p> <div class="highlight"><pre><code class="html"><span class="nt">&lt;img</span> <span class="na">src=</span><span class="s">&quot;img.jpg&quot;</span> <span class="na">srcset=</span><span class="s">&quot;img-3.0.jpg 3x, img-2.0.jpg 2x,</span> <span class="s"> img-1.5.jpg 1.5x, img-1.0.jpg 1x&quot;</span> <span class="nt">/&gt;</span> </code></pre></div> <div class="highlight"><pre><code class="css"><span class="nf">#someid</span> <span class="p">{</span> <span class="k">background-image</span><span class="o">:</span> <span class="n">image</span><span class="o">-</span><span class="n">set</span><span class="p">(</span> <span class="sx">url(img-3.0.jpg)</span> <span class="m">3</span><span class="n">x</span><span class="o">,</span> <span class="sx">url(img-2.0.jpg)</span> <span class="m">2</span><span class="n">x</span><span class="o">,</span> <span class="sx">url(img-2.0.jpg)</span> <span class="m">1</span><span class="o">.</span><span class="m">5</span><span class="n">x</span><span class="o">,</span> <span class="sx">url(img-1.0.jpg)</span> <span class="m">1</span><span class="n">x</span><span class="p">)</span> <span class="p">}</span> </code></pre></div> <p>Call me crazy, but that seems silly. I know web developers relish writing repeated boilerplate code, and better, inflating our markup with complex-looking syntax because it helps our job security, but... <code>&lt;/sarcasm&gt;</code>, perhaps there is an easier way? How about:</p> <div class="highlight"><pre><code class="html"><span class="nt">&lt;img</span> <span class="na">src=</span><span class="s">&quot;img.jpg&quot;</span><span class="nt">&gt;</span> </code></pre></div> <div class="highlight"><pre><code class="css"><span class="nf">#someid</span> <span class="p">{</span> <span class="k">background-image</span><span class="o">:</span> <span class="sx">url(img.jpg)</span> <span class="p">}</span> </code></pre></div> <p>Yes, the old-fashioned <code>img</code> tag and <code>url()</code> could do just fine while delivering all the same benefits as the example above, but without the bloat, and even better, without requiring any changes to how we write our code today. The browser can do all the work on our behalf and HTTP already provides the mechanisms to handle this.</p> <h2>Client-Hints and DPR negotiation</h2> <blockquote>Client Hints can be used as input to proactive content negotiation; just as the Accept header allowed clients to indicate what formats they prefer, Client Hints allow clients to indicate a list of device and agent specific preferences.</blockquote> <p>You can find the <a href="http://tools.ietf.org/html/draft-grigorik-http-client-hints-00">full spec on the IETF tracker</a>, but the above makes it sound way more complicated than it really is. In a nutshell, just as we can now <a href="http://www.igvita.com/2013/05/01/deploying-webp-via-accept-content-negotiation/">use the Accept header</a> to determine which image format the client supports, the <code>CH-DPR</code> (aka Client-Hints DPR) header communicates the DPR resolution of the device to the server, and the server provides the appropriate asset. In HTTP speak, this work as follows:</p> <div class="highlight"><pre><code class="bash"><span class="o">(</span>request<span class="o">)</span> GET /img.jpg HTTP/1.1 User-Agent: Awesome Browser Accept: image/webp, image/jpg CH-DPR: 2.0 <span class="o">(</span>response<span class="o">)</span> HTTP/1.1 200 OK Server: Awesome Server Content-Type: image/jpg Content-Length: 124523 Vary: CH-DPR DPR: 2.0 <span class="o">(</span>image data<span class="o">)</span> </code></pre></div> <p>The client sends the <code>CH-DPR: 2.0</code> hint to the server, and the server can choose to use this information to serve the approtiate image. If the server opts-out, or ignores the the CH-DPR header, then it can serve the default image - no harm done. But if it does elect to use the <code>CH-DPR</code> hint, then it also appends the <code>Vary: CH-DPR</code> header to indicate to upstream caches that the value of CH-DPR header should be used as part of the cache key. Nothing new, moving along.</p> <h2>Hands on with Client-Hints and Nginx</h2> <p>It doesn't take much to implement a "DPR aware" server. For the sake of an example, lets do just that with Nginx:</p> <div class="highlight"><pre><code class="nginx"><span class="k">http</span> <span class="p">{</span> <span class="c1"># - capture the DPR resolution from CH-DPR header</span> <span class="c1"># - if no CH-DPR header is present, default to 1.0</span> <span class="c1"># - round DPR ranges to neares available number asset (1.0, 1.5, 2.0)</span> <span class="kn">map</span> <span class="nv">$http_ch_dpr</span> <span class="nv">$dpr</span> <span class="p">{</span> <span class="kn">default</span> <span class="s">&quot;1.0&quot;</span><span class="p">;</span> <span class="kn">~1\.[01234]</span> <span class="s">&quot;1.0&quot;</span><span class="p">;</span> <span class="kn">~1\.[56789]</span> <span class="s">&quot;1.5&quot;</span><span class="p">;</span> <span class="kn">~2\.[0123456789]</span> <span class="s">&quot;2.0&quot;</span><span class="p">;</span> <span class="p">}</span> <span class="kn">server</span> <span class="p">{</span> <span class="c1"># - check for appropriate DPR asset</span> <span class="c1"># - if appropriate DPR is not available, fallback to original</span> <span class="kn">location</span> <span class="p">~</span><span class="sr">*</span> <span class="s">/images/(?&lt;name&gt;.*)\.(?&lt;ext&gt;.*)</span>$ <span class="p">{</span> <span class="kn">add_header</span> <span class="s">DPR</span> <span class="nv">$dpr</span><span class="p">;</span> <span class="kn">add_header</span> <span class="s">Vary</span> <span class="s">CH-DPR</span><span class="p">;</span> <span class="kn">try_files</span> <span class="s">/images/</span><span class="nv">$name-$dpr.$ext</span> <span class="nv">$uri</span> <span class="p">=</span><span class="mi">404</span><span class="p">;</span> <span class="p">}</span> <span class="c1"># ... regular nginx configuration</span> <span class="p">}</span> <span class="p">}</span> </code></pre></div> <p>Seven lines of code, that's all it takes. First, the <code>map</code> directive captures the DPR value out of the <code>CH-DPR</code> header, and set is to "1.0" if missing. Then, we create a new location directive which intercepts request to <code>/images/</code> folder and rewrite the request by looking up the appropriate file on disk. If no such file is found, we try the original file, and finally 404 if that is missing as well. Let's test it out:</p> <div class="highlight"><pre><code class="bash"><span class="nv">$&gt;</span> curl -s http://localhost:8080/images/awesome.jpg | wc -c 252680 <span class="nv">$&gt;</span> curl -s -H<span class="s2">&quot;CH-DPR: 1.5&quot;</span> http://localhost:8080/images/awesome.jpg | wc -c 381135 <span class="nv">$&gt;</span> curl -s -H<span class="s2">&quot;CH-DPR: 2.0&quot;</span> http://localhost:8080/images/awesome.jpg | wc -c 710175 </code></pre></div> <p>The first request does not contain the <code>CH-DPR</code> header and the server returns the default (1.0) asset. In the second request, we add the <code>CH-DPR</code> header with DPR value set to "1.5" and a different image is served, which you can see by the byte-count. Magic! Except, it's not. Just vanilla HTTP doing exactly what it was designed for.</p> <p>How did Nginx produce the right DPR asset? In this case, I simply pre-generated the right assets and placed them on disk: <em>awesome-1.0.jpg</em>, <em>awesome-1.5.jpg</em>, and <em>awesome-2.0.jpg</em>. It doesn't take much to add a build step using a tool like grunt to <a href="http://addyosmani.com/blog/generate-multi-resolution-images-for-srcset-with-grunt/">automate this part</a> of your workflow. Alternatively, the request could have been proxied to an image service (local or third-party), which can create these assets dynamically. Point being, this is automation at work. I like automation; I'm lazy; I have more important problems to solve than to repeat boilerplate markup.</p> <p><em>Update, Oct 30, 2013: check out <a href="https://github.com/igrigorik/http-client-hints/tree/master/server">sample reference CH-DPR server</a> on GitHub.</em></p> <h2>But, but, but...</h2> <p><img src='http://www.igvita.com//www.igvita.com/posts/12/picket.png' class='left' /><span class="red"><strong>But this puts more header bytes on the wire!</strong></span> Yes, it does. Repeated boilerplate markup puts <em>a lot more</em> bytes on the wire. Granted, the HTTP 1.x header bytes are uncompressed, but even with that the CH-DPR header will add a whopping 11 bytes. Once you repeat all of the markup boilerplate for every DPR breakpoint, for every image, CH-DPR will come out ahead by a good margin. Client-Hints will save bytes.</p> <p><span class="red"><strong>But...</strong></span> this will fragment the cache, and Vary support is "missing", and old clients don't work well with Vary? I've tackled all of these previously <a href="http://www.igvita.com/2012/12/18/deploying-new-image-formats-on-the-web/">when talking about Accept negotiation</a>. Since then, we've had multiple CDN's <a href="http://blog.netdna.com/developer/how-to-reduce-image-size-with-webp-automagically/">successfully</a> <a href="http://cloudinary.com/blog/how_to_support_webp_images_save_bandwidth_and_improve_user_performance">deploy</a> Accept negotiation for WebP delivery, and we now have big services (including at Google) relying on Accept negotiation.</p> <p><span class="red"><strong>But we can automate boilerplate generation!</strong></span> Yes, we could. Extra build steps could parse the HTML and CSS and rewrite it to inject the appropriate markup. I'll let you be the judge whether that's a "better solution". Having said that, I'm <a href="https://groups.google.com/a/chromium.org/d/msg/blink-dev/dA8lbqLvaXA/b39vij5VPoMJ">not opposed to implementing srcset</a>, I just happen to think that Client-Hints addresses the problem in a more elegant way.</p> <p><span class="red"><strong>But my server doesn't do X!</strong></span> Get a better server. Yes, I'm serious. That's a much cheaper solution in the long run - your time is more valuable. Further, if Client-Hints is adopted, you can bet that there will be dozens of plugins and modules available in no time flat to automate the entire problem. As we saw above, it doesn't take much.</p> <h2>Simplifying the "responsive images" discussion</h2> <p>DPR switching is only one part of the larger "responsive images" discussion: we also have resolution switching, art-direction, and an increasing chorus of people asking for "pixel perfect" images (i.e. images scaled to exact size in the viewport to avoid client resizing). These are independent problems and we should treat them as such.</p> <p>In fact, I would argue that the only solution that requires explicit markup is the art-direction case - by definition, that's up to the designer to dictate. The rest is just boilerplate that can and should be automated. I don't want to think about this stuff, it should just work. HTTP can help.</p> <div class="feedflare"> <a href="http://feeds.igvita.com/~ff/igvita?a=019fSVpyQq4:1zTKGqHGZps:yIl2AUoC8zA"><img src="http://feeds.feedburner.com/~ff/igvita?d=yIl2AUoC8zA" border="0"></img></a> </div><img src="http://feeds.feedburner.com/~r/igvita/~4/019fSVpyQq4" height="1" width="1"/>
</content>
<feedburner:origLink>
http://www.igvita.com/2013/08/29/automating-dpr-switching-with-client-hints/
</feedburner:origLink>
</entry>
<entry>
<title type="html">HTTP Archive + BigQuery = Web Performance Answers</title>
<link rel="alternate" type="text/html" href="http://feeds.igvita.com/~r/igvita/~3/uDo94--3uRw/"/>
<updated>2013-06-20T00:00:00-07:00</updated>
<id>
http://www.igvita.com/2013/06/20/http-archive-bigquery-web-performance-answers/
</id>
<content type="html">
<p><img src='http://www.igvita.com/posts/13/ha-bigquery.png' class='left' /><a href="http://httparchive.org/">HTTP Archive</a> is a treasure trove of web performance data. Launched in late 2010, the project crawls over 300,000 most popular sites twice a month and records how the web is built: number and types of resources, size of each resource, whether the resources are compressed or marked as cacheable, times to render the page, time to first paint, and so on - you get the point.</p> <p>The HTTP Archive site itself provides a number <a href="http://httparchive.org/interesting.php">interesting stats</a> and <a href="http://httparchive.org/trends.php">aggregate trends</a>, but the data on the site only scratches the surface of the kinds of questions you can ask! To satisfy your curiousity, all you need to do is download and import ~400GB of raw SQL/CSV data. Easy, right? Yeah, not really. Instead, <strong>wouldn't it be nice if we had the full dataset of all the HTTP Archive data to query on demand, and with ad-hoc questions?</strong></p> <h2>Google BigQuery + HTTP Archive</h2> <div class='ytvideo' id='bhUMHKJf3r4'></div> <p>Well, good news, now you can satisfy your curiosity in minutes (or seconds, even). The full HTTP Archive dataset is now available on BigQuery! To get started, <a href="https://developers.google.com/bigquery/sign-up">signup for BigQuery</a> and head to <a href="https://bigquery.cloud.google.com">bigquery.cloud.google.com</a> and click the down arrow beside "API Project": <code>Switch to project > Display project > enter "httparchive"</code></p> <p><img src='http://www.igvita.com/posts/13/bigquery-instructions.png' class='center' style='max-width:695px;width:100%;' /></p> <p>Once the project is imported, expand the project in left navigation and you'll see a collection of tables for individual pages and request data for each run of the HTTP Archive crawler. From there, pull up the SQL console, and go nuts! For example, let's start simple: <strong>what is the median time to first render?</strong></p> <div class="highlight"><pre><code class="sql"><span class="k">SELECT</span> <span class="n">NTH</span><span class="p">(</span><span class="mi">50</span><span class="p">,</span> <span class="n">quantiles</span><span class="p">(</span><span class="n">renderStart</span><span class="p">,</span><span class="mi">100</span><span class="p">))</span> <span class="n">median</span><span class="p">,</span> <span class="n">NTH</span><span class="p">(</span><span class="mi">75</span><span class="p">,</span> <span class="n">quantiles</span><span class="p">(</span><span class="n">renderStart</span><span class="p">,</span><span class="mi">100</span><span class="p">))</span> <span class="n">seventy_fifth</span><span class="p">,</span> <span class="n">NTH</span><span class="p">(</span><span class="mi">90</span><span class="p">,</span> <span class="n">quantiles</span><span class="p">(</span><span class="n">renderStart</span><span class="p">,</span><span class="mi">100</span><span class="p">))</span> <span class="n">ninetieth</span> <span class="k">FROM</span> <span class="p">[</span><span class="n">httparchive</span><span class="p">:</span><span class="n">runs</span><span class="p">.</span><span class="mi">2013</span><span class="n">_06_01_pages</span><span class="p">]</span> </code></pre></div> <p>And the answer is: <strong>2.2s median, 3.3s for 75th percentile, and 4.7s for 90th percentile</strong>. BigQuery provides a number of convenience functions, such as quantiles, statistical approximations, extended regular expression matching and extraction, and a lot more. Check out the <a href="https://developers.google.com/bigquery/docs/query-reference">query reference</a> and <a href="https://developers.google.com/bigquery/docs/query-cookbook">query cookbook</a> to learn more.</p> <h2>One JS framework is not enough!</h2> <p>To flex our new BigQuery muscle let's find pages which use multiple JavaScript frameworks on same page:</p> <div class="highlight"><pre><code class="sql"><span class="k">SELECT</span> <span class="n">pages</span><span class="p">.</span><span class="n">pageid</span><span class="p">,</span> <span class="n">url</span><span class="p">,</span> <span class="n">cnt</span><span class="p">,</span> <span class="n">libs</span><span class="p">,</span> <span class="n">pages</span><span class="p">.</span><span class="n">rank</span> <span class="n">rank</span> <span class="k">FROM</span> <span class="p">[</span><span class="n">httparchive</span><span class="p">:</span><span class="n">runs</span><span class="p">.</span><span class="mi">2013</span><span class="n">_06_01_pages</span><span class="p">]</span> <span class="k">as</span> <span class="n">pages</span> <span class="k">JOIN</span> <span class="p">(</span> <span class="k">SELECT</span> <span class="n">pageid</span><span class="p">,</span> <span class="k">count</span><span class="p">(</span><span class="k">distinct</span><span class="p">(</span><span class="k">type</span><span class="p">))</span> <span class="n">cnt</span><span class="p">,</span> <span class="n">GROUP_CONCAT</span><span class="p">(</span><span class="k">type</span><span class="p">)</span> <span class="n">libs</span> <span class="k">FROM</span> <span class="p">(</span> <span class="k">SELECT</span> <span class="n">REGEXP_EXTRACT</span><span class="p">(</span><span class="n">url</span><span class="p">,</span> <span class="n">r</span><span class="s1">&#39;(jquery|dojo|angular|prototype|backbone|emberjs|sencha|scriptaculous).*\.js&#39;</span><span class="p">)</span> <span class="k">type</span><span class="p">,</span> <span class="n">pageid</span> <span class="k">FROM</span> <span class="p">[</span><span class="n">httparchive</span><span class="p">:</span><span class="n">runs</span><span class="p">.</span><span class="mi">2013</span><span class="n">_06_01_requests</span><span class="p">]</span> <span class="k">WHERE</span> <span class="n">REGEXP_MATCH</span><span class="p">(</span><span class="n">url</span><span class="p">,</span> <span class="n">r</span><span class="s1">&#39;jquery|dojo|angular|prototype|backbone|emberjs|sencha|scriptaculous.*\.js&#39;</span><span class="p">)</span> <span class="k">GROUP</span> <span class="k">BY</span> <span class="n">pageid</span><span class="p">,</span> <span class="k">type</span> <span class="p">)</span> <span class="k">GROUP</span> <span class="k">BY</span> <span class="n">pageid</span> <span class="k">HAVING</span> <span class="n">cnt</span> <span class="o">&gt;=</span> <span class="mi">2</span> <span class="p">)</span> <span class="k">as</span> <span class="n">lib</span> <span class="k">ON</span> <span class="n">lib</span><span class="p">.</span><span class="n">pageid</span> <span class="o">=</span> <span class="n">pages</span><span class="p">.</span><span class="n">pageid</span> <span class="k">WHERE</span> <span class="n">rank</span> <span class="k">IS</span> <span class="k">NOT</span> <span class="k">NULL</span> <span class="k">ORDER</span> <span class="k">BY</span> <span class="n">rank</span> <span class="k">asc</span> </code></pre></div> <p><img src='http://www.igvita.com/posts/13/ha-js-frameworks.png' class='center' style='max-width:618px;width:100%;' /></p> <p>JQuery is the clear crowd favorite, but prototype (despite its age) is holding strong. Worse, there are many sites with three or four different frameworks, and even different versions of each one on the same page!</p> <p>Happy <a href="http://research.google.com/pubs/pub36632.html">dremeling</a> - err, <em>BigQuerying!</em> For more examples, check out this <a href="https://gist.github.com/igrigorik/5801492">gist with sample queries</a>.</p> <div class="feedflare"> <a href="http://feeds.igvita.com/~ff/igvita?a=uDo94--3uRw:X5XugPXr-YI:yIl2AUoC8zA"><img src="http://feeds.feedburner.com/~ff/igvita?d=yIl2AUoC8zA" border="0"></img></a> </div><img src="http://feeds.feedburner.com/~r/igvita/~4/uDo94--3uRw" height="1" width="1"/>
</content>
<feedburner:origLink>
http://www.igvita.com/2013/06/20/http-archive-bigquery-web-performance-answers/
</feedburner:origLink>
</entry>
</feed>
</div>
</div>
<div id="center" style="background-color:silver;">
<div>
This XML file does not appear to have any style information associated with it. The document tree is shown below.
<rss xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:media="http://search.yahoo.com/mrss/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:nyt="http://www.nytimes.com/namespaces/rss/2.0" version="2.0">
<channel>
<title>Room for Debate | New York Times</title>
<link>http://www.nytimes.com/roomfordebate</link>
<description>
A Running Commentary on the News â The New York Times
</description>
<copyright>Copyright 2014 The New York Times Company</copyright>
<webMaster>feedback@nytimes.com (New York Times)</webMaster>
<pubDate>Sat, 13 Sep 2014 00:25:55 -0400</pubDate>
<item>
<media:group>
<media:content url="http://graphics8.nytimes.com/images/2014/09/11/opinion/RFD-Male-Minority-Teachers/RFD-Male-Minority-Teachers-thumbStandard.jpg" medium="image" height="75" width="75"/>
<media:content url="http://graphics8.nytimes.com/images/2014/09/11/opinion/RFD-Male-Minority-Teachers/RFD-Male-Minority-Teachers-articleInline.jpg" medium="image" height="126" width="190"/>
</media:group>
<title>How to Diversify Teaching</title>
<link>
http://www.nytimes.com/roomfordebate/2014/09/11/how-to-diversify-teaching
</link>
<guid>
http://www.nytimes.com/roomfordebate/2014/09/11/how-to-diversify-teaching
</guid>
<description>
<p/>
<p>
The teaching profession is
<a href="http://www.nytimes.com/2014/09/07/sunday-review/why-dont-more-men-go-into-teaching.html?_r=0">dominated by women</a>
: Three-quarters of all teachers in kindergarten through high school are female, and in elementary and middle schools, women account for more than 80 percent of the educators.
<br/>
<br/>
What's more,
<a href="http://cdn.americanprogress.org/wp-content/uploads/2014/06/Partee-TeachersOfColor-report2.pdf">
more than 80 percent of Americaâs teachers are white
</a>
, even though minority students are
<a href="http://m.theatlantic.com/education/archive/2014/08/this-fall-minorities-will-outnumber-white-students-in-us-schools/378772/href=">expected to outnumber white students</a>
in public schools for the first time this year.
<br/>
<br/>
How can the teaching profession become more diverse in terms of gender and race or ethnicity?
</p>
</description>
<content:encoded>
<![CDATA[
<p></p> <p>The teaching profession is <a href="http://www.nytimes.com/2014/09/07/sunday-review/why-dont-more-men-go-into-teaching.html?_r=0">dominated by women</a>: Three-quarters of all teachers in kindergarten through high school are female, and in elementary and middle schools, women account for more than 80 percent of the educators. <br /> <br />What's more, <a href="http://cdn.americanprogress.org/wp-content/uploads/2014/06/Partee-TeachersOfColor-report2.pdf">more than 80 percent of Americaâs teachers are white</a>, even though minority students are <a href="http://m.theatlantic.com/education/archive/2014/08/this-fall-minorities-will-outnumber-white-students-in-us-schools/378772/href=">expected to outnumber white students</a> in public schools for the first time this year. <br /> <br />How can the teaching profession become more diverse in terms of gender and race or ethnicity?</p> <p><strong>Responses:</strong></p> <p><a href="http://www.nytimes.com/roomfordebate/2014/09/11/how-to-diversify-teaching/pay-teachers-better-and-support-new-recruits">Pay Teachers Better and Support New Recruits</a> <br/> Deborah L. Voltz<em>, Dean of School of Education</em> </p> <p><a href="http://www.nytimes.com/roomfordebate/2014/09/11/how-to-diversify-teaching/very-few-jobs-besides-teaching-offer-this-much-reward">Very Few Jobs Offer This Much Reward</a> <br/> Jeremy Greensmith<em>, fourth grade teacher</em> </p> <p><a href="http://www.nytimes.com/roomfordebate/2014/09/11/how-to-diversify-teaching/with-teachers-the-best-talent-means-diverse-talent">The Best Talent Means Diverse Talent</a> <br/> Elisa Villanueva Beard<em>, Teach for America</em> </p> <p><a href="http://www.nytimes.com/roomfordebate/2014/09/11/how-to-diversify-teaching/the-teaching-profession-provides-little-room-for-growth">The Profession Provides Little Room for Growth</a> <br/> Mariana Souto-Manning<em>, Professor of Education</em> </p> <p><a href="http://www.nytimes.com/roomfordebate/2014/09/11/how-to-diversify-teaching/to-diversify-the-teaching-profession-address-the-underlying-issues">Address the Underlying Issues</a> <br/> Karolyn Belcher<em>, TNTP</em> </p> <p><a href="http://www.nytimes.com/roomfordebate/2014/09/11/how-to-diversify-teaching/make-teaching-attractive-and-accessible-to-black-males">Make It Attractive and Accessible</a> <br/> Roy Jones<em>, Call Me Mister</em> </p>
]]>
</content:encoded>
<pubDate>Thu, 11 Sep 2014 17:53:56 -0400</pubDate>
<lastBuildDate>Fri, 12 Sep 2014 18:25:58 -0400</lastBuildDate>
</item>
<item>
<media:group>
<media:content url="http://graphics8.nytimes.com/images/2014/09/10/opinion/RFD-Domestic-Violence/RFD-Domestic-Violence-thumbStandard.jpg" medium="image" height="75" width="75"/>
<media:content url="http://graphics8.nytimes.com/images/2014/09/10/opinion/RFD-Domestic-Violence/RFD-Domestic-Violence-articleInline.jpg" medium="image" height="126" width="190"/>
</media:group>
<title>Going After the Abusers</title>
<link>
http://www.nytimes.com/roomfordebate/2014/09/10/going-after-abusers-like-nfl-player-ray-rice
</link>
<guid>
http://www.nytimes.com/roomfordebate/2014/09/10/going-after-abusers-like-nfl-player-ray-rice
</guid>
<description>
<p/>
<p>
The Baltimore Raven's
<a href="http://www.nytimes.com/2014/09/09/sports/football/ray-rice-video-shows-punch-and-raises-new-questions-for-nfl.html">decision</a>
this week to terminate Ray Riceâs contract because of his assault on his then-girlfriend Janay Palmer, coincides with the 20th anniversary of the Violence Against Women Act, which aims to hold abusers accountable. To combat domestic violence, some states have adopted mandatory arrest and âno-dropâ prosecution policies.
</p>
<p>
Are these policies effective? When police have evidence of domestic violence, should prosecution be mandatory?
</p>
</description>
<content:encoded>
<![CDATA[
<p></p> <p>The Baltimore Raven's <a href="http://www.nytimes.com/2014/09/09/sports/football/ray-rice-video-shows-punch-and-raises-new-questions-for-nfl.html">decision</a> this week to terminate Ray Riceâs contract because of his assault on his then-girlfriend Janay Palmer, coincides with the 20th anniversary of the Violence Against Women Act, which aims to hold abusers accountable. To combat domestic violence, some states have adopted mandatory arrest and âno-dropâ prosecution policies.</p> <p>Are these policies effective? When police have evidence of domestic violence, should prosecution be mandatory?</p> <p><strong>Responses:</strong></p> <p><a href="http://www.nytimes.com/roomfordebate/2014/09/10/going-after-abusers-like-nfl-player-ray-rice/healthy-alternatives-to-prosecution-can-help-victims">There Are Healthy Alternatives to Prosecution</a> <br/> Leigh Goodmark<em>, law professor</em> </p> <p><a href="http://www.nytimes.com/roomfordebate/2014/09/10/going-after-abusers-like-nfl-player-ray-rice/treat-domestic-violence-like-the-crime-it-is">Treat It Like the Crime It Is</a> <br/> Ayonna Johnson<em>, Womenâs Resource Center to End Domestic Violence</em> </p> <p><a href="http://www.nytimes.com/roomfordebate/2014/09/10/going-after-abusers-like-nfl-player-ray-rice/mandatory-prosecutions-wouldnt-change-violent-behavior">It Wouldn&#x27;t Change Violent Behavior</a> <br/> Charlie Stoops<em>, Co-Founder, Center for Advancing Domestic Peace</em> </p> <p><a href="http://www.nytimes.com/roomfordebate/2014/09/10/going-after-abusers-like-nfl-player-ray-rice/mandatory-policies-can-be-a-threat-to-women">Mandatory Policies Can Be a Threat to Women</a> <br/> Donna Coker<em>, law professor</em> </p> <p><a href="http://www.nytimes.com/roomfordebate/2014/09/10/going-after-abusers-like-nfl-player-ray-rice/mandatory-prosecution-puts-the-burden-rightly-on-the-state">Prosecution Puts the Burden on the State, Not the Victim</a> <br/> Yolanda Jimenez<em>, Joe Torre Safe At Home Foundation</em> </p>
]]>
</content:encoded>
<pubDate>Wed, 10 Sep 2014 18:54:50 -0400</pubDate>
<lastBuildDate>Thu, 11 Sep 2014 16:00:30 -0400</lastBuildDate>
</item>
<item>
<media:group>
<media:content url="http://graphics8.nytimes.com/images/2014/09/09/opinion/RFD-wages-for-housework/RFD-wages-for-housework-thumbStandard.jpg" medium="image" height="75" width="75"/>
<media:content url="http://graphics8.nytimes.com/images/2014/09/09/opinion/RFD-wages-for-housework/RFD-wages-for-housework-articleInline.jpg" medium="image" height="126" width="190"/>
</media:group>
<title>Wages for Housework</title>
<link>
http://www.nytimes.com/roomfordebate/2014/09/09/wages-for-housework
</link>
<guid>
http://www.nytimes.com/roomfordebate/2014/09/09/wages-for-housework
</guid>
<description>
<p/>
<p>
Housework is a necessary labor for families, but it is largely unpaid, except when others are hired to do it. Families may pay others to cook, clean or take care of their children, but they donât pay themselves. This year,
<a href="http://www.theguardian.com/world/2014/mar/07/italian-campaigners-housewives-paid-salary">Italy</a>
considered a proposal in which the government, or in some cases the husband or partner, would pay wives for this thankless task. And a few years ago,
<a href="http://globalvoicesonline.org/2012/09/14/india-husbands-to-pay-wives-for-doing-household-chores/">India</a>
considered a similar bill.
</p>
<p>
Should the family member who does most of the housekeeping be compensated?
</p>
</description>
<content:encoded>
<![CDATA[
<p></p> <p>Housework is a necessary labor for families, but it is largely unpaid, except when others are hired to do it. Families may pay others to cook, clean or take care of their children, but they donât pay themselves. This year, <a href="http://www.theguardian.com/world/2014/mar/07/italian-campaigners-housewives-paid-salary">Italy</a> considered a proposal in which the government, or in some cases the husband or partner, would pay wives for this thankless task. And a few years ago, <a href="http://globalvoicesonline.org/2012/09/14/india-husbands-to-pay-wives-for-doing-household-chores/">India</a> considered a similar bill. </p> <p>Should the family member who does most of the housekeeping be compensated? </p> <p><strong>Responses:</strong></p> <p><a href="http://www.nytimes.com/roomfordebate/2014/09/09/wages-for-housework/taking-unpaid-housework-for-granted-is-wrong">Taking Unpaid Housework for Granted Is Wrong</a> <br/> Noah Zatz<em>, law professor</em> </p> <p><a href="http://www.nytimes.com/roomfordebate/2014/09/09/wages-for-housework/a-better-solution-to-household-chores-work-family-balance">A Better Solution: Work-Family Balance</a> <br/> Heather Boushey<em>, Washington Center for Equitable Growth</em> </p> <p><a href="http://www.nytimes.com/roomfordebate/2014/09/09/wages-for-housework/give-women-equitable-opporunity-in-the-public-sphere">Give Women Equitable Opportunity </a> <br/> Milad Doroudian<em>, writer </em> </p> <p><a href="http://www.nytimes.com/roomfordebate/2014/09/09/wages-for-housework/compensation-for-housework-should-be-a-family-decision">Compensation Should be a Family Decision</a> <br/> Porcshe N. Moran<em>, Journalist</em> </p>
]]>
</content:encoded>
<pubDate>Tue, 09 Sep 2014 19:20:20 -0400</pubDate>
<lastBuildDate>Wed, 10 Sep 2014 06:46:21 -0400</lastBuildDate>
</item>
<item>
<media:group>
<media:content url="http://graphics8.nytimes.com/images/2014/09/08/opinion/RFD-Nato-and-Ukraine/RFD-Nato-and-Ukraine-thumbStandard.jpg" medium="image" height="75" width="75"/>
<media:content url="http://graphics8.nytimes.com/images/2014/09/08/opinion/RFD-Nato-and-Ukraine/RFD-Nato-and-Ukraine-articleInline.jpg" medium="image" height="126" width="190"/>
</media:group>
<title>NATO Challenge on the Edge of Its Mandate</title>
<link>
http://www.nytimes.com/roomfordebate/2014/09/08/should-nato-be-helping-ukraine-face-russia
</link>
<guid>
http://www.nytimes.com/roomfordebate/2014/09/08/should-nato-be-helping-ukraine-face-russia
</guid>
<description>
<p/>
<p>
While President Obama
<a href="http://www.nytimes.com/2014/09/04/world/europe/obama-calls-russia-ukraine-moves-brazen-assault.html">condemned</a>
Russia's intervention in Ukraine and East European nations
<a href="http://www.nytimes.com/2014/09/06/world/europe/in-eastern-europe-ukraine-crisis-focuses-minds-on-nato-and-military-spending.html">called</a>
for a more assertive response from NATO, the alliance at its summit last week announced only
<a href="http://www.nytimes.com/2014/09/06/world/europe/nato-summit.html">limited steps</a>
to support the nonmember nation. Meanwhile, three former U.S. ambassadors to Moscow are
<a href="http://www.nytimes.com/2014/09/09/opinion/give-diplomacy-with-russia-a-chance.html?_r=0">urging</a>
restraint.
</p>
<p>
Should NATO be defending a nonmember nation? Are European leaders in a position to help Ukraine?
</p>
</description>
<content:encoded>
<![CDATA[
<p></p> <p>While President Obama <a href="http://www.nytimes.com/2014/09/04/world/europe/obama-calls-russia-ukraine-moves-brazen-assault.html">condemned</a> Russia's intervention in Ukraine and East European nations <a href="http://www.nytimes.com/2014/09/06/world/europe/in-eastern-europe-ukra
]]>
<![CDATA[
ine-crisis-focuses-minds-on-nato-and-military-spending.html">called</a> for a more assertive response from NATO, the alliance at its summit last week announced only <a href="http://www.nytimes.com/2014/09/06/world/europe/nato-summit.html">limited steps</a> to support the nonmember nation. Meanwhile, three former U.S. ambassadors to Moscow are <a href="http://www.nytimes.com/2014/09/09/opinion/give-diplomacy-with-russia-a-chance.html?_r=0">urging</a> restraint.</p> <p>Should NATO be defending a nonmember nation? Are European leaders in a position to help Ukraine? </p> <p><strong>Responses:</strong></p> <p><a href="http://www.nytimes.com/roomfordebate/2014/09/08/should-nato-be-helping-ukraine-face-russia/stay-out-of-ukraine-but-deploy-eastward">Stay Out of Ukraine, but Deploy Eastward</a> <br/> Andrew J. Bacevich<em>, Boston University</em> </p> <p><a href="http://www.nytimes.com/roomfordebate/2014/09/08/should-nato-be-helping-ukraine-face-russia/ukraine-needs-and-deserves-support-from-nato-and-the-west">Ukraine Needs and Deserves Support</a> <br/> Celeste Ward Gventer<em>, Former Pentagon official</em> </p> <p><a href="http://www.nytimes.com/roomfordebate/2014/09/08/should-nato-be-helping-ukraine-face-russia/the-west-is-wrong-to-see-putin-as-the-enemy">Stop Seeing Putin as the Enemy</a> <br/> Boris Kagarlitsky<em>, Transnational Institute</em> </p> <p><a href="http://www.nytimes.com/roomfordebate/2014/09/08/should-nato-be-helping-ukraine-face-russia/nato-should-not-be-defending-a-nonmember-country">Don&#x27;t Defend a Nonmember</a> <br/> Jeffrey Sommers<em>, Stockholm School of Economics in Riga</em> </p> <p><a href="http://www.nytimes.com/roomfordebate/2014/09/08/should-nato-be-helping-ukraine-face-russia/there-are-other-ways-to-help-ukraine">There Are Other Ways to Help</a> <br/> Ian Bond<em>, Centre for European Reform, London</em> </p> <p><a href="http://www.nytimes.com/roomfordebate/2014/09/08/should-nato-be-helping-ukraine-face-russia/nato-should-act-in-europes-defense-not-ukraines">To Europe&#x27;s Defense, Not Ukraine&#x27;s</a> <br/> Alexandra de Hoop Scheffer<em>, German Marshall Fund</em> </p> <p><a href="http://www.nytimes.com/roomfordebate/2014/09/08/should-nato-be-helping-ukraine-face-russia/nato-must-stop-russian-aggression-in-ukraine">NATO Must Stop Russia</a> <br/> Artis Pabriks<em>, Latvian politician</em> </p>
]]>
</content:encoded>
<pubDate>Mon, 08 Sep 2014 19:02:12 -0400</pubDate>
<lastBuildDate>Wed, 10 Sep 2014 16:02:48 -0400</lastBuildDate>
</item>
<item>
<media:group>
<media:content url="http://graphics8.nytimes.com/images/2014/09/05/opinion/RFDfashionweek/RFDfashionweek-thumbStandard.jpg" medium="image" height="75" width="75"/>
<media:content url="http://graphics8.nytimes.com/images/2014/09/05/opinion/RFDfashionweek/RFDfashionweek-articleInline.jpg" medium="image" height="126" width="190"/>
</media:group>
<title>Who Owns Fashion?</title>
<link>
http://www.nytimes.com/roomfordebate/2014/09/07/who-owns-fashion
</link>
<guid>
http://www.nytimes.com/roomfordebate/2014/09/07/who-owns-fashion
</guid>
<description>
<p/>
<p>
It's
<a href="http://www.nytimes.com/interactive/2014/fashion/fashion-week-now-spring-2015.html?ref=fashion">Fashion Week</a>
in New York City and insiders and fans alike have flocked to witness the seasonâs new designs. Many of the
<a href="http://www.nytimes.com/2012/11/11/magazine/how-zara-grew-into-the-worlds-largest-fashion-retailer.html?pagewanted=all">"fast fashion"</a>
retailers are present as well, ready to manufacture similar styles at a mass-market scale.
</p>
<p>
But where is the difference between mimicking a trend and theft? Some advocate for better legal protection for original designs. Do fashion designers need better copyright protection?
</p>
<p>
Giselle Defares, a writer in Amsterdam, suggested this discussion.
</p>
</description>
<content:encoded>
<![CDATA[
<p></p> <p>It's <a href="http://www.nytimes.com/interactive/2014/fashion/fashion-week-now-spring-2015.html?ref=fashion">Fashion Week</a> in New York City and insiders and fans alike have flocked to witness the seasonâs new designs. Many of the <a href="http://www.nytimes.com/2012/11/11/magazine/how-zara-grew-into-the-worlds-largest-fashion-retailer.html?pagewanted=all">"fast fashion"</a> retailers are present as well, ready to manufacture similar styles at a mass-market scale. </p> <p>But where is the difference between mimicking a trend and theft? Some advocate for better legal protection for original designs. Do fashion designers need better copyright protection?</p> <p>Giselle Defares, a writer in Amsterdam, suggested this discussion.</p> <p><strong>Responses:</strong></p> <p><a href="http://www.nytimes.com/roomfordebate/2014/09/07/who-owns-fashion/lines-blur-with-inspiration-and-theft">Lines Blur With Inspiration and Theft</a> <br/> Giselle Defares<em>, writer</em> </p> <p><a href="http://www.nytimes.com/roomfordebate/2014/09/07/who-owns-fashion/fashion-designers-need-strong-legal-protection-for-their-clothing">Designers Need Legal Protection</a> <br/> Susan Scafidi<em>, Fashion Law Institute</em> and Narciso Rodriguez<em>, Fashion Designer</em> </p> <p><a href="http://www.nytimes.com/roomfordebate/2014/09/07/who-owns-fashion/piracy-fuels-the-fashion-industry">Piracy Fuels the Industry</a> <br/> Kal Raustiala<em></em> and Christopher Sprigman<em>, Authors, &quot;The Knockoff Economy&quot;</em> </p> <p><a href="http://www.nytimes.com/roomfordebate/2014/09/07/who-owns-fashion/emulate-the-european-model">Emulate the European Model</a> <br/> Guillermo C. Jimenez<em>, Fashion Institute of Technology</em> </p>
]]>
</content:encoded>
<pubDate>Sun, 07 Sep 2014 19:00:00 -0400</pubDate>
<lastBuildDate>Mon, 08 Sep 2014 18:38:33 -0400</lastBuildDate>
</item>
<item>
<media:group>
<media:content url="http://graphics8.nytimes.com/images/2014/09/05/opinion/rfdvacation/rfdvacation-thumbStandard.jpg" medium="image" height="75" width="75"/>
<media:content url="http://graphics8.nytimes.com/images/2014/09/05/opinion/rfdvacation/rfdvacation-articleInline.jpg" medium="image" height="126" width="190"/>
</media:group>
<title>Why Don't Americans Take Vacation?</title>
<link>
http://www.nytimes.com/roomfordebate/2014/09/04/why-dont-americans-take-vacation-7
</link>
<guid>
http://www.nytimes.com/roomfordebate/2014/09/04/why-dont-americans-take-vacation-7
</guid>
<description>
<p/>
<p>
According to
<a href="http://traveleffect.com/sites/traveleffect.com/files/OverwhelmedAmerica_FullReport_FINAL_0.pdf">a new report</a>
, four in 10 American workers allow some of their paid vacation days to go unused.
</p>
<p>
Why aren't we taking time off? Is it because we're a culture of workaholics or are companies not doing enough to accommodate paid vacation?
</p>
</description>
<content:encoded>
<![CDATA[
<p></p> <p>According to <a href="http://traveleffect.com/sites/traveleffect.com/files/OverwhelmedAmerica_FullReport_FINAL_0.pdf">a new report</a>, four in 10 American workers allow some of their paid vacation days to go unused.</p> <p>Why aren't we taking time off? Is it because we're a culture of workaholics or are companies not doing enough to accommodate paid vacation?</p> <p><strong>Responses:</strong></p> <p><a href="http://www.nytimes.com/roomfordebate/2014/09/04/why-dont-americans-take-vacation-7/many-feel-trapped-by-work">Many Feel Trapped by Work</a> <br/> John de Graaf<em>, Take Back Your Time</em> </p> <p><a href="http://www.nytimes.com/roomfordebate/2014/09/04/why-dont-americans-take-vacation-7/we-need-standards-for-paid-time-off">We Need Standards for Paid Time Off</a> <br/> Ellen Bravo<em>, Family Values @ Work</em> </p> <p><a href="http://www.nytimes.com/roomfordebate/2014/09/04/why-dont-americans-take-vacation-7/overworking-is-part-of-our-identity">Overworking Is Part of Our Identity</a> <br/> Adam Okulicz-Kozaryn<em>, Rutgers University</em> </p> <p><a href="http://www.nytimes.com/roomfordebate/2014/09/04/why-dont-americans-take-vacation-7/unlimited-vacation-so-crazy-it-works">Unlimited Vacation -- So Crazy It Works</a> <br/> Bruce Elliott<em>, Society for Human Resource Management</em> </p> <p><a href="http://www.nytimes.com/roomfordebate/2014/09/04/why-dont-americans-take-vacation-7/tweets-on-vacation-or-lack-thereof">Tweets on Vacation, or Lack Thereof </a> <br/> Readers <em></em> </p>
]]>
</content:encoded>
<pubDate>Thu, 04 Sep 2014 18:12:00 -0400</pubDate>
<lastBuildDate>Fri, 05 Sep 2014 17:32:20 -0400</lastBuildDate>
</item>
<item>
<media:group>
<media:content url="http://graphics8.nytimes.com/images/2014/09/04/opinion/rfdgoodell/rfdgoodell-thumbStandard.jpg" medium="image" height="75" width="75"/>
<media:content url="http://graphics8.nytimes.com/images/2014/09/04/opinion/rfdgoodell/rfdgoodell-articleInline.jpg" medium="image" height="126" width="190"/>
</media:group>
<title>Tax Break for Sports</title>
<link>
http://www.nytimes.com/roomfordebate/2014/09/03/should-pro-sport-leagues-get-tax-breaks
</link>
<guid>
http://www.nytimes.com/roomfordebate/2014/09/03/should-pro-sport-leagues-get-tax-breaks
</guid>
<description>
<p/>
<p>
The league offices and associations for several professional sports are tax-exempt nonprofit groups. The teams themselves pay taxes on their revenue, but critics say the leagues are lucrative enough to pay for their administrative expenses without a tax break, especially since
<a href="http://www.nytimes.com/2014/02/15/sports/football/goodell-nfl-commissioner-earned-44-2-million-in-2012.html">some executives get enormous compensation</a>
.
</p>
<p>
Should pro sports leagues enjoy nonprofit status or are taxpayers paying too many of their bills?
</p>
</description>
<content:encoded>
<![CDATA[
<p></p> <p>The league offices and associations for several professional sports are tax-exempt nonprofit groups. The teams themselves pay taxes on their revenue, but critics say the leagues are lucrative enough to pay for their administrative expenses without a tax break, especially since <a href="http://www.nytimes.com/2014/02/15/sports/football/goodell-nfl-commissioner-earned-44-2-million-in-2012.html">some executives get enormous compensation</a>.</p> <p>Should pro sports leagues enjoy nonprofit status or are taxpayers paying too many of their bills?</p> <p><strong>Responses:</strong></p> <p><a href="http://www.nytimes.com/roomfordebate/2014/09/03/should-pro-sport-leagues-get-tax-breaks/end-the-nfl-tax-breaks">End the N.F.L. Tax Breaks</a> <br/> Ryan Alexander<em>, Taxpayers for Common Sense</em> </p> <p><a href="http://www.nytimes.com/roomfordebate/2014/09/03/should-pro-sport-leagues-get-tax-breaks/the-nonprofit-status-of-sports-leagues-is-irrelevant">Nonprofit Status Is Irrelevant</a> <br/> Andrew Zimbalist<em>, economist, Smith College</em> </p> <p><a href="http://www.nytimes.com/roomfordebate/2014/09/03/should-pro-sport-leagues-get-tax-breaks/changing-the-nonprofit-status-of-sports-leagues-wouldnt-bring-in-more-money">It Might Not Bring In Any More Money</a> <br/> Judith Grant Long<em>, Professor of Sports Management</em> </p> <p><a href="http://www.nytimes.com/roomfordebate/2014/09/03/should-pro-sport-leagues-get-tax-breaks/sports-leagues-do-not-need-taxpayer-help">Sports Leagues Do Not Need Taxpayer Help</a> <br/> Patrick Hruby<em>, Journalist</em> </p> <p><a href="http://www.nytimes.com/roomfordebate/2014/09/03/should-pro-sport-leagues-get-tax-breaks/nonprofit-teams-rather-than-leagues">Nonprofit Teams Rather Than Leagues?</a> <br/> Richard Steinberg<em>, professor of economics</em> </p>
]]>
</content:encoded>
<pubDate>Wed, 03 Sep 2014 18:00:32 -0400</pubDate>
<lastBuildDate>Thu, 04 Sep 2014 15:30:32 -0400</lastBuildDate>
</item>
<item>
<media:group>
<media:content url="http://graphics8.nytimes.com/images/2014/09/02/opinion/RFD-US-foreign-policy/RFD-US-foreign-policy-thumbStandard.jpg" medium="image" height="75" width="75"/>
<media:content url="http://graphics8.nytimes.com/images/2014/09/02/opinion/RFD-US-foreign-policy/RFD-US-foreign-policy-articleInline.jpg" medium="image" height="126" width="190"/>
</media:group>
<title>
Is 'Don't Do Stupid Stuff' the Best Foreign Policy?
</title>
<link>
http://www.nytimes.com/roomfordebate/2014/09/02/is-dont-do-stupid-stuff-the-best-foreign-policy-30
</link>
<guid>
http://www.nytimes.com/roomfordebate/2014/09/02/is-dont-do-stupid-stuff-the-best-foreign-policy-30
</guid>
<description>
<p/>
<p>
Last week when President Obama said,
<a href="http://www.nytimes.com/2014/08/29/world/obama-vows-new-russia-penalties-over-ukraine.html">"We don't have a strategy yet</a>
" on dealing with ISIS in Syria, many said it was another sign of indecision when what was needed was forceful action and that, as
<a href="http://www.theatlantic.com/international/archive/2014/08/hillary-clinton-failure-to-help-syrian-rebels-led-to-the-rise-of-isis/375832/">Hillary Clinton said</a>
derisively, his foreign policy objective was simply "don't do stupid stuff."
</p>
<p>
But is caution and deliberation rather than quick action the best way to serve the national interest in a treacherous global landscape? Is "Don't do stupid stuff" the best foreign policy?
</p>
</description>
<content:encoded>
<![CDATA[
<p></p> <p>Last week when President Obama said, <a href="http://www.nytimes.com/2014/08/29/world/obama-vows-new-russia-penalties-over-ukraine.html">"We don't have a strategy yet</a>" on dealing with ISIS in Syria, many said it was another sign of indecision when what was needed was forceful action and that, as <a href="http://www.theatlantic.com/international/archive/2014/08/hillary-clinton-failure-to-help-syrian-rebels-led-to-the-rise-of-isis/375832/">Hillary Clinton said</a> derisively, his foreign policy objective was simply "don't do stupid stuff." </p> <p>But is caution and deliberation rather than quick action the best way to serve the national interest in a treacherous global landscape? Is "Don't do stupid stuff" the best foreign policy? </p> <p><strong>Responses:</strong></p> <p><a href="http://www.nytimes.com/roomfordebate/2014/09/02/is-dont-do-stupid-stuff-the-best-foreign-policy-30/obama-shows-a-lack-of-faith-in-american-power">A Lack of Faith in American Power</a> <br/> Shadi Hamid<em>, Brookings Institution</em> </p> <p><a href="http://www.nytimes.com/roomfordebate/2014/09/02/is-dont-do-stupid-stuff-the-best-foreign-policy-30/obama-realizes-that-war-should-be-a-last-resort">War Should Be a Last Resort</a> <br/> Heather Parton<em>, Blogger</em> </p> <p><a href="http://www.nytimes.com/roomfordebate/2014/09/02/is-dont-do-stupid-stuff-the-best-foreign-policy-30/the-consequences-of-obamas-dithering">The Consequences of Dithering</a> <br/> Kori Schake<em>, Hoover Institution</em> </p> <p><a href="http://www.nytimes.com/roomfordebate/2014/09/02/is-dont-do-stupid-stuff-the-best-foreign-policy-30/history-shows-caution-is-the-best-approach-for-foreign-action">History Shows Caution Is the Best Approach</a> <br/> Stephen M. Walt<em>, Harvard University</em> </p>
]]>
</content:encoded>
<pubDate>Tue, 02 Sep 2014 18:10:01 -0400</pubDate>
<lastBuildDate>Wed, 03 Sep 2014 14:15:08 -0400</lastBuildDate>
</item>
<item>
<media:group>
<media:content url="http://graphics8.nytimes.com/images/2014/08/30/opinion/rfdpolice/rfdpolice-thumbStandard.jpg" medium="image" height="75" width="75"/>
<media:content url="http://graphics8.nytimes.com/images/2014/08/30/opinion/rfdpolice/rfdpolice-thumbWide.jpg" medium="image" height="126" width="190"/>
</media:group>
<title>Black and White and Blue</title>
<link>
http://www.nytimes.com/roomfordebate/2014/09/01/black-and-white-and-blue
</link>
<guid>
http://www.nytimes.com/roomfordebate/2014/09/01/black-and-white-and-blue
</guid>
<description>
<p/>
<p>
The shooting of Michael Brown, an unarmed black man, by a white police officer in Ferguson, Mo., ignited a debate about the frequency with which African-Americans are stopped, arrested and even shot by the police. The data on
<a href="http://www.nytimes.com/2014/08/31/sunday-review/race-and-police-shootings-are-blacks-targeted-more.html?ref=opinion">police shootings</a>
appears inconclusive, but many say the frequency with which black men are
<a href="http://www.theatlantic.com/national/archive/2014/08/the-problem-is-im-black/379357/">harassed by police</a>
alone demonstrates the persistence of
<a href="http://www.vox.com/2014/8/28/6051971/police-implicit-bias-michael-brown-ferguson-missouri/in/5757650">racism</a>
.
</p>
<p>
Is racism an intractable problem for the police, or can other factors explain the disparate rate at which African-Americans are stopped and arrested?
</p>
</description>
<content:encoded>
<![CDATA[
<p></p> <p>The shooting of Michael Brown, an unarmed black man, by a white police officer in Ferguson, Mo., ignited a debate about the frequency with which African-Americans are stopped, arrested and even shot by the police. The data on <a href="http://www.nytimes.com/2014/08/31/sunday-review/race-and-police-shootings-are-blacks-targeted-more.html?ref=opinion">police shootings</a> appears inconclusive, but many say the frequency with which black men are <a href="http://www.theatlantic.com/national/archive/2014/08/the-problem-is-im-black/379357/">harassed by police</a> alone demonstrates the persistence of <a href="http://www.vox.com/2014/8/28/6051971/police-implicit-bias-michael-brown-ferguson-missouri/in/5757650">racism</a>. </p> <p>Is racism an intractable problem for the police, or can other factors explain the disparate rate at which African-Americans are stopped and arrested? </p> <p><strong>Responses:</strong></p> <p><a href="http://www.nytimes.com/roomfordebate/2014/09/01/black-and-white-and-blue/better-training-transparency-and-accountability-could-tackle-racial-bias">Training, Transparency and Accountability</a> <br/> Kami Chavis Simmons<em>, former federal prosecutor</em> </p> <p><a href="http://www.nytimes.com/roomfordebate/2014/09/01/black-and-white-and-blue/bias-is-universal-awareness-can-assure-justice">Bias Is Universal. Awareness Can Assure Justice</a> <br/> Neill Franklin<em>, Former police officer</em> </p> <p><a href="http://www.nytimes.com/roomfordebate/2014/09/01/black-and-white-and-blue/in-assessing-police-racism-note-racial-disparity-in-criminal-activity">Racial Differences in Criminal Activity Must Be Noted</a> <br/> Alfred Blumstein<em>, Sociologist</em> </p> <p><a href="http://www.nytimes.com/roomfordebate/2014/09/01/black-and-white-and-blue/enlist-young-cops-to-fight-the-demon-of-racism">Enlist Young Cops to Fight the Demon</a> <br/> Eugene O&#x27;Donnell<em>, John Jay College of Criminal Justice</em> </p> <p><a href="http://www.nytimes.com/roomfordebate/2014/09/01/black-and-white-and-blue/to-combat-racism-in-law-enforcement-start-young">To Combat Racism, Start With Children</a> <br/> Katheryn Russell-Brown<em>, author, &quot;The Color of Crime&quot;</em> </p> <p><a href="http://www.nytimes.com/roomfordebate/2014/09/01/black-and-white-and-blue/other-real-factors-can-obscure-the-role-in-racism-in-policing">Other Real Factors Can Obscure It</a> <br/> Ivan Yihshyan Sun<em>, Sociologist</em> </p> <p><a href="http://www.nytimes.com/roomfordebate/2014/09/01/black-and-white-and-blue/racially-targeted-law-enforcement-is-the-problem">Racially Targeted Enforcement Is the Problem</a> <br/> David A. Harris<em>, author, &quot;Failed Evidence&quot;</em> </p>
]]>
</content:encoded>
<pubDate>Mon, 01 Sep 2014 16:48:31 -0400</pubDate>
<lastBuildDate>Tue, 02 Sep 2014 12:29:07 -0400</lastBuildDate>
</item>
<item>
<media:group>
<media:content url="http://graphics8.nytimes.com/images/2014/08/29/opinion/rfdradicals/rfdradicals-thumbStandard.jpg" medium="image" height="75" width="75"/>
<media:content url="http://graphics8.nytimes.com/images/2014/08/29/opinion/rfdradicals/rfdradicals-articleInline.jpg" medium="image" height="126" width="190"/>
</media:group>
<title>Homegrown Terrorists and the West</title>
<link>
http://www.nytimes.com/roomfordebate/2014/08/28/how-to-stop-radicalization-in-the-west
</link>
<guid>
http://www.nytimes.com/roomfordebate/2014/08/28/how-to-stop-radicalization-in-the-west
</guid>
<description>
<p/>
<p>
The ISIS fighter wielding the knife in the video showing the murder of James Foley is reportedly part of a growing
<a href="http://www.nytimes.com/2014/08/25/world/middleeast/britain-james-foley-isis.html">group of British jihadists in Syria</a>
. And the first American
<a href="http://www.nytimes.com/2014/08/27/world/middleeast/american-fighting-for-isis-is-killed-in-syria.html">is believed to have been killed</a>
fighting for the militant group. These events have drawn renewed attention to the dangers posed by radicalized young Muslims in the United States and Europe.
</p>
<p>
What can be done to stop young Muslims in the West from adopting radical views and joining militant groups?
</p>
</description>
<content:encoded>
<![CDATA[
<p></p> <p>The ISIS fighter wielding the knife in the video showing the murder of James Foley is reportedly part of a growing <a href="http://www.nytimes.com/2014/08/25/world/middleeast/britain-james-foley-isis.html">group of British jihadists in Syria</a>. And the first American <a href="http://www.nytimes.com/2014/08/27/world/middleeast/american-fighting-for-isis-is-killed-in-syria.html">is believed to have been killed</a> fighting for the militant group. These events have drawn renewed attention to the dangers posed by radicalized young Muslims in the United States and Europe.</p> <p>What can be done to stop young Muslims in the West from adopting radical views and joining militant groups? </p> <p><strong>Responses:</strong></p> <p><a href="http://www.nytimes.com/roomfordebate/2014/08/28/how-to-stop-radicalization-in-the-west/challenge-radicals-loudly-and-clearly">Challenge Radicals Loudly and Clearly</a> <br/> Ghaffar Hussain<em>, Quilliam Foundation</em> </p> <p><a href="http://www.nytimes.com/roomfordebate/2014/08/28/how-to-stop-radicalization-in-the-west/europe-needs-to-embrace-islam">Integration Beyond Socioeconomic Opportunity</a> <br/> Jocelyne Cesari<em>, Islam in the West Program, Harvard University</em> </p> <p><a href="http://www.nytimes.com/roomfordebate/2014/08/28/how-to-stop-radicalization-in-the-west/there-are-ways-to-address-radicalism-early">Address Radicalism Early</a> <br/> Raffaello Pantucci<em>, Royal United Services Institute</em> </p> <p><a href="http://www.nytimes.com/roomfordebate/2014/08/28/how-to-stop-radicalization-in-the-west/battle-of-ideas-moves-online">Battle of Ideas Moves Online</a> <br/> Jamie Bartlett<em>, Author, &quot;The Dark Net&quot;</em> </p> <p><a href="http://www.nytimes.com/roomfordebate/2014/08/28/how-to-stop-radicalization-in-the-west/fluid-situation-with-no-single-solution">Fluid Situation With No Single Solution</a> <br/> Patrick M. Skinner<em>, Soufan Group, former C.I.A. officer</em> </p>
]]>
</content:encoded>
<pubDate>Thu, 28 Aug 2014 18:15:00 -0400</pubDate>
<lastBuildDate>Tue, 02 Sep 2014 13:04:19 -0400</lastBuildDate>
</item>
</channel>
</rss>
</div>
</div>
<div style="float:right;">

</div>
<nav class="menu" id="first"><a href="page1.html">Page One</a></nav>
<nav class="menu"><a href="page2.html">Page Two</a></nav>
<nav class="menu"><a href="index_code.html">Example</a></nav>
<nav class="menu"></nav>
<nav class="menu"></nav>
<nav class="menu"></nav>
<nav class="menu"></nav>
<nav class="menu"></nav>
<nav class="menu"></nav>
<nav class="menu"></nav>
<nav class="menu"></nav>


</div><!--endcontainer-->


</body>
</html>
